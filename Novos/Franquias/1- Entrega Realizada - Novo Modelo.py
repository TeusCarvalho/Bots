import os
import requests
import warnings
import polars as pl
import pandas as pd
import multiprocessing
import logging
from datetime import datetime, timedelta, date

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# ==========================================================
# CONFIGURA√á√ÉO DE LOGGING
# ==========================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("sla_franquias.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# ==========================================================
# VARI√ÅVEIS DE CONFIGURA√á√ÉO
# ==========================================================
os.environ["POLARS_MAX_THREADS"] = str(multiprocessing.cpu_count())

PASTA_ENTRADA = r"C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda (1)\√Årea de Trabalho\Testes\14-  SLA Entrega Realizada Franquia"
WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/92a82aea-9b5c-4e3d-9169-8d4753ecef38"
LINK_PASTA = "https://jtexpressdf-my.sharepoint.com/:f:/g/personal/matheus_carvalho_jtexpressdf_onmicrosoft_com/EvIP3oIiLJRAqcB1SZ_1nmYBXLIYSJkIns5Pf_Xz2OqY_w?e=OEXsJN"

COL_DATA_ORIGINAL = "Data prevista de entrega"
COL_DATA_UPPER = "DATA PREVISTA DE ENTREGA"
COL_DATA_REF = "DATA_REF"

# Ative para logar diferen√ßas de schema por arquivo
DIAGNOSTICO_SCHEMA = True

# ==========================================================
# LISTA DE BASES V√ÅLIDAS
# ==========================================================
BASES_VALIDAS = [
    'F CHR-AM', 'F CAC-RO', 'F PDR-GO','F PVH-RO','F ARQ - RO','F AGB-MT', 'F GYN 03-GO','MAO -AM', 'RBR 02-AC', 'F RBR-AC', 'IPR -GO',
    'F GYN - GO','F VHL-RO', 'F PON-GO', 'F ANP-GO','F GYN 02-GO','F CDN-AM', 'F AGL-GO','F APG - GO', 'F RVD - GO', 'F PDT-TO',
    'F PLN-DF','F SEN-GO','F PVL-MT', 'F TRD-GO', 'F CEI-DF','F CNF-MT', 'F FMA-GO','F ALV-AM','F POS-GO','F PPA-MS','F MAC-AP', 'F GAI-TO',
    'F CRX-GO', 'F DOM -PA', 'F CCR-MT', 'F GRP-TO', 'F PVL 02-MT','F AMB-MS','F BVB-RR','F SVC-RR', 'F MCP-AP','F JPN 02-RO', 'F MCP 02-AP','F BSL-AC',
    'F PVH 02-RO', 'F JPN-RO', 'F CMV-MT','F DOU-MS','F PGM-PA', 'F RDC -PA', 'F XIG-PA','F TGT-DF','F CGR - MS',
    'F VLP-GO', 'F CGR 02-MS','F PLA-GO', 'F TGA-MT','F RFI-DF', 'F ORL-PA', 'F ITI-PA',
    'F PCA-PA','F CNC-PA','F SJA-GO', 'F IGA-PA','F PAZ-AM','F TUR-PA','F JCD-PA', 'F TLA-PA','F ELD-PA', 'F BSB-DF', 'F OCD-GO',
    'F EMA-DF', 'F GUA-DF','F STM-PA', 'F SBN-DF',
]

# ==========================================================
# HELPERS (NORMALIZA√á√ÉO / SCHEMA)
# ==========================================================
def _make_unique_columns(cols: list[str]) -> list[str]:
    """
    Garante nomes √∫nicos de colunas (caso alguma planilha venha com duplicatas).
    Ex: ["A","A"] -> ["A","A__2"]
    """
    seen = {}
    out = []
    for c in cols:
        base = (c if c is not None else "").strip()
        if base == "":
            base = "COL"
        if base not in seen:
            seen[base] = 1
            out.append(base)
        else:
            seen[base] += 1
            out.append(f"{base}__{seen[base]}")
    return out


def _normalize_columns(df: pl.DataFrame) -> pl.DataFrame:
    """
    Normaliza nomes: strip + UPPER e resolve duplicatas.
    """
    if df.is_empty():
        return df

    cols_norm = []
    for c in df.columns:
        c2 = (c if c is not None else "").strip().upper()
        cols_norm.append(c2 if c2 != "" else "COL")

    cols_unique = _make_unique_columns(cols_norm)

    if cols_unique != df.columns:
        df = df.rename({old: new for old, new in zip(df.columns, cols_unique)})

    return df


def _align_schemas(dfs: list[pl.DataFrame]) -> list[pl.DataFrame]:
    """
    Faz a uni√£o de colunas entre todos dfs e adiciona colunas faltantes como NULL,
    retornando todos com a MESMA ordem de colunas.
    """
    if not dfs:
        return dfs

    # Ordem determin√≠stica: come√ßa pelo primeiro df e vai adicionando novas colunas que aparecerem
    all_cols: list[str] = []
    seen = set()
    for df in dfs:
        for c in df.columns:
            if c not in seen:
                seen.add(c)
                all_cols.append(c)

    aligned = []
    for df in dfs:
        missing = [c for c in all_cols if c not in df.columns]
        if missing:
            df = df.with_columns([pl.lit(None).alias(c) for c in missing])
        df = df.select(all_cols)
        aligned.append(df)

    return aligned


# ==========================================================
# FUN√á√ïES DE PROCESSAMENTO DE DADOS
# ==========================================================
def ler_planilha_rapido(caminho: str) -> pl.DataFrame:
    """L√™ um arquivo (Excel ou CSV) de forma r√°pida e segura + normaliza colunas."""
    try:
        if caminho.lower().endswith(".csv"):
            df = pl.read_csv(caminho)
        else:
            df = pl.read_excel(caminho)

        df = _normalize_columns(df)
        return df

    except Exception as e:
        logging.error(f"Erro ao ler {caminho}: {e}")
        return pl.DataFrame()


def consolidar_planilhas(pasta: str) -> pl.DataFrame:
    """L√™ e consolida arquivos de forma sequencial e alinha schemas para evitar erro no concat."""
    arquivos = [
        os.path.join(pasta, f)
        for f in os.listdir(pasta)
        if f.lower().endswith((".xlsx", ".xls", ".csv")) and not f.startswith("~$")
    ]

    if not arquivos:
        raise FileNotFoundError("Nenhum arquivo encontrado na pasta de entrada.")

    dfs: list[pl.DataFrame] = []
    schemas_info = []  # (arquivo, n_cols, cols)

    logging.info(f"üìÇ Encontrados {len(arquivos)} arquivos. Iniciando leitura sequencial...")

    for i, arquivo in enumerate(arquivos):
        nome = os.path.basename(arquivo)
        logging.info(f"Lendo arquivo {i + 1}/{len(arquivos)}: {nome}...")

        df = ler_planilha_rapido(arquivo)

        if not df.is_empty():
            # Coluna auxiliar p/ rastrear origem (ajuda muito no debug)
            df = df.with_columns(pl.lit(nome).alias("_ARQUIVO_ORIGEM"))
            dfs.append(df)
            schemas_info.append((nome, len(df.columns), df.columns))
        else:
            logging.warning(f"Arquivo ignorado (vazio ou erro de leitura): {nome}")

    if not dfs:
        raise ValueError("Nenhum DataFrame v√°lido foi lido dos arquivos.")

    # Diagn√≥stico r√°pido de schema
    if DIAGNOSTICO_SCHEMA:
        base_nome, base_n, base_cols = schemas_info[0]
        for nome, n, cols in schemas_info[1:]:
            if n != base_n:
                logging.warning(f"‚ö†Ô∏è Schema diferente: {nome} tem {n} colunas (base {base_nome} tem {base_n}).")
            # Tamb√©m loga diferen√ßas por nome (mesmo com mesma contagem)
            set_base = set(base_cols)
            set_cols = set(cols)
            missing = sorted(list(set_base - set_cols))
            extra = sorted(list(set_cols - set_base))
            if missing or extra:
                if missing:
                    logging.warning(f"   - {nome} faltando colunas: {missing[:20]}{'...' if len(missing) > 20 else ''}")
                if extra:
                    logging.warning(f"   - {nome} colunas extras: {extra[:20]}{'...' if len(extra) > 20 else ''}")

    logging.info("üîÑ Todos os arquivos lidos. Alinhando schemas (uni√£o de colunas) ...")
    dfs = _align_schemas(dfs)

    logging.info("üîÑ Schemas alinhados. Iniciando concatena√ß√£o...")
    # Agora pode usar vertical_relaxed com seguran√ßa (mesma largura)
    df_final = pl.concat(dfs, how="vertical_relaxed")

    logging.info(f"üìÇ Base consolidada com {df_final.height} linhas e {len(df_final.columns)} colunas.")
    return df_final


def preparar_coluna_data(df: pl.DataFrame) -> pl.DataFrame:
    """Padroniza e converte a coluna de data para o tipo Date."""
    df = _normalize_columns(df)

    if COL_DATA_UPPER not in df.columns:
        raise KeyError(f"Coluna '{COL_DATA_ORIGINAL}' n√£o encontrada no DataFrame. (Esperado: '{COL_DATA_UPPER}')")

    temp_col = "temp_data_str"

    df = df.with_columns(
        pl.col(COL_DATA_UPPER)
        .cast(pl.Utf8)
        .str.strip_chars()
        .str.replace(r" .*$", "")
        .alias(temp_col)
    )

    formatos_data = ["%d/%m/%Y", "%Y-%m-%d", "%d-%m-%Y", "%Y/%m/%d", "%Y%m%d"]
    tentativas_parse = [pl.col(temp_col).str.strptime(pl.Date, fmt, strict=False) for fmt in formatos_data]

    df = df.with_columns(pl.coalesce(tentativas_parse).alias(COL_DATA_REF)).drop(temp_col)
    df = df.filter(pl.col(COL_DATA_REF).is_not_null())

    logging.info(f"üìÖ Datas convertidas e validadas. Restam {df.height} linhas.")
    return df


def calcular_sla(df: pl.DataFrame) -> pd.DataFrame | None:
    """
    Calcula o SLA por base de entrega.
    Retorna um DataFrame com o resumo ou None se a coluna de prazo n√£o for encontrada.
    """
    df = _normalize_columns(df)

    if "BASE DE ENTREGA" not in df.columns:
        logging.warning("Coluna 'BASE DE ENTREGA' n√£o encontrada. N√£o ser√° poss√≠vel calcular SLA.")
        return None

    possiveis_nomes_coluna = ["ENTREGUE NO PRAZO?", "ENTREGUE NO PRAZOÔºü"]
    col_upper = [c.upper() for c in df.columns]

    col_prazo = next((df.columns[i] for i, nome in enumerate(col_upper) if nome in possiveis_nomes_coluna), None)

    if col_prazo is None:
        logging.warning("Coluna 'ENTREGUE NO PRAZO?' n√£o encontrada. N√£o ser√° poss√≠vel calcular SLA para este DataFrame.")
        return None

    df = df.with_columns(
        pl.when(pl.col(col_prazo).cast(pl.Utf8).str.to_uppercase() == "Y")
        .then(1)
        .otherwise(0)
        .alias("_ENTREGUE")
    )

    resumo = (
        df.group_by("BASE DE ENTREGA")
        .agg([
            pl.len().alias("Total"),
            pl.col("_ENTREGUE").sum().alias("No Prazo"),
            (pl.len() - pl.col("_ENTREGUE").sum()).alias("Fora"),
            (pl.col("_ENTREGUE").sum() / pl.len()).alias("SLA"),
        ])
        .sort("SLA")
    )

    r = resumo.to_pandas()
    r.rename(columns={"BASE DE ENTREGA": "Base De Entrega"}, inplace=True)
    return r


# ==========================================================
# FUN√á√ÉO PRINCIPAL (COM RELAT√ìRIO SIMPLIFICADO)
# ==========================================================
def exibir_e_enviar_card(
    resumo_mes: pd.DataFrame,
    resumo_domingos: pd.DataFrame | None,
    primeiro_dia: date,
    ultimo_dia: date
):
    """Exibe o relat√≥rio simplificado no terminal e o envia para o Feishu."""
    try:
        logging.info("üì§ Preparando relat√≥rio simplificado...")

        piores_df_mes = resumo_mes.sort_values(by="SLA", ascending=True).head(4)
        periodo_str = f"{primeiro_dia.strftime('%d/%m')} a {ultimo_dia.strftime('%d/%m')}"
        data_atual_str = datetime.now().strftime("%d/%m/%Y")

        conteudo_piores = (
            f"üö® **Alerta de SLA ‚Äî Franquias**\n"
            f"**Atualizado em:** {data_atual_str}\n"
            f"**üìâ 4 Piores Bases ‚Äî {periodo_str}**\n\n"
        )
        for _, row in piores_df_mes.iterrows():
            sla_percent = row["SLA"] * 100
            conteudo_piores += f"{row['Base De Entrega']} | SLA: {sla_percent:.2f}%\n"

        if resumo_domingos is None or resumo_domingos.empty:
            conteudo_domingos = f"\n**üìä Domingos do m√™s ‚Äî Nenhuma base registrada.**"
        else:
            piores_df_domingos = resumo_domingos.sort_values(by="SLA", ascending=True).head(4)
            conteudo_domingos = (
                f"\n**üìâ 4 Piores Bases ‚Äî Domingos do m√™s ({primeiro_dia.strftime('%m/%Y')})**\n\n"
            )
            for _, row in piores_df_domingos.iterrows():
                sla_percent = row["SLA"] * 100
                conteudo_domingos += f"{row['Base De Entrega']} | SLA: {sla_percent:.2f}%\n"

        conteudo_final = conteudo_piores + conteudo_domingos

        print("\n" + "=" * 80)
        print("üìä RELAT√ìRIO DE SLA - VISUALIZA√á√ÉO LOCAL")
        print("=" * 80)
        print(conteudo_final)
        print("=" * 80)
        print("Enviando este relat√≥rio para o Feishu...")
        print("=" * 80 + "\n")

        msg = {
            "msg_type": "interactive",
            "card": {
                "config": {"wide_screen_mode": True},
                "header": {
                    "template": "red",
                    "title": {"tag": "plain_text", "content": f"SLA Franquias ‚Äî {periodo_str}"}
                },
                "elements": [
                    {"tag": "markdown", "content": conteudo_final},
                    {
                        "tag": "action",
                        "actions": [
                            {
                                "tag": "button",
                                "text": {"tag": "plain_text", "content": "üìÅ Abrir Pasta dos Relat√≥rios"},
                                "type": "primary",
                                "url": LINK_PASTA
                            }
                        ]
                    }
                ]
            }
        }

        response = requests.post(WEBHOOK_URL, json=msg, timeout=15)
        response_data = response.json()

        if response.status_code == 200 and response_data.get("code") == 0:
            logging.info("‚úÖ Card enviado com sucesso para o Feishu!")
        else:
            logging.error(f"‚ùå Erro ao enviar card para o Feishu. Status: {response.status_code}, Resposta: {response.text}")

    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Erro de conex√£o com o Feishu: {e}")
    except Exception as e:
        logging.error(f"‚ùå Erro inesperado ao enviar card: {e}", exc_info=True)


# ==========================================================
# BLOCO PRINCIPAL DE EXECU√á√ÉO (MENSAL + DEDUP)
# ==========================================================
if __name__ == "__main__":
    try:
        logging.info("üöÄ Iniciando script de SLA v12.4 (Fix schema lengths differ)...")

        # 1. Processar os dados
        df_consolidado = consolidar_planilhas(PASTA_ENTRADA)
        df_preparado = preparar_coluna_data(df_consolidado)

        df_filtrado = df_preparado.with_columns(
            pl.col("BASE DE ENTREGA").cast(pl.Utf8).str.to_uppercase().str.strip_chars()
        ).filter(
            pl.col("BASE DE ENTREGA").is_in([b.upper() for b in BASES_VALIDAS])
        )

        # 2. Remover duplicatas
        linhas_antes = df_filtrado.height
        df_filtrado = df_filtrado.unique(keep="first")
        linhas_depois = df_filtrado.height
        duplicatas_removidas = linhas_antes - linhas_depois

        if duplicatas_removidas > 0:
            logging.info(f"üßπ Foram encontradas e removidas {duplicatas_removidas} linhas duplicadas.")
        else:
            logging.info("‚úÖ Nenhuma linha duplicada encontrada nos dados.")

        if df_filtrado.is_empty():
            raise ValueError("Nenhuma linha restante ap√≥s o filtro de bases v√°lidas e remo√ß√£o de duplicatas.")

        # 3. Definir per√≠odo de an√°lise (M√äS INTEIRO)
        data_ref = df_filtrado.select(pl.col(COL_DATA_REF)).max().item()
        primeiro_dia = data_ref.replace(day=1)

        if data_ref.month == 12:
            proximo_mes = date(data_ref.year + 1, 1, 1)
        else:
            proximo_mes = date(data_ref.year, data_ref.month + 1, 1)
        ultimo_dia = proximo_mes - timedelta(days=1)

        logging.info(
            f"üìÜ Per√≠odo de an√°lise: M√™s de {primeiro_dia.strftime('%m/%Y')} "
            f"({primeiro_dia.strftime('%d/%m/%Y')} a {ultimo_dia.strftime('%d/%m/%Y')})"
        )

        # 4. SLA do m√™s
        df_mes = df_filtrado.filter(pl.col(COL_DATA_REF).is_between(primeiro_dia, ultimo_dia))
        if df_mes.is_empty():
            raise ValueError("Sem dados para o m√™s atual.")

        resumo_mes = calcular_sla(df_mes)
        if resumo_mes is None:
            raise ValueError("N√£o foi poss√≠vel calcular o SLA para o m√™s.")

        # 5. SLA domingos
        df_domingos = df_mes.filter(pl.col(COL_DATA_REF).dt.weekday() == 6)  # 6 = Domingo
        resumo_domingos = calcular_sla(df_domingos) if not df_domingos.is_empty() else None

        # 6. Exibir + enviar card
        exibir_e_enviar_card(resumo_mes, resumo_domingos, primeiro_dia, ultimo_dia)

        logging.info("üèÅ Processo finalizado com sucesso.")

    except Exception as e:
        logging.critical(f"‚ùå ERRO FATAL: {e}", exc_info=True)
