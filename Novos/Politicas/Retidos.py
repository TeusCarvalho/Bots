# -*- coding: utf-8 -*-
import os
import json
import re
import logging
import polars as pl
# CORRE√á√ÉO 1: Importar timedelta junto com datetime
from datetime import datetime, timedelta
import requests


# ============================================================
# üß© FUN√á√ïES AUXILIARES (GLOBAIS)
# ============================================================

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            # Verifique se este caminho de log est√° correto para voc√™
            logging.FileHandler("../../Antigos/analise_retidos.log", encoding="utf-8"),
            logging.StreamHandler()
        ]
    )


def converter_datetime(df: pl.DataFrame, coluna: str) -> pl.DataFrame:
    if coluna not in df.columns:
        return df

    formatos = [
        "%Y-%m-%d %H:%M:%S", "%d/%m/%Y %H:%M:%S",
        "%Y-%m-%d %H:%M", "%d/%m/%Y %H:%M",
        "%Y-%m-%d", "%d/%m/%Y", "%Y%m%d"
    ]

    for fmt in formatos:
        try:
            newdf = df.with_columns(
                pl.col(coluna).str.strptime(pl.Datetime, fmt, strict=False)
            )
            if newdf[coluna].is_not_null().any():
                logging.info(f"‚úîÔ∏è Coluna '{coluna}' convertida com sucesso usando o formato {fmt}.")
                return newdf
        except Exception as e:
            pass

    logging.warning(f"‚ö†Ô∏è Falha ao converter a coluna '{coluna}' com todos os formatos conhecidos.")
    return df


def detectar_coluna(df: pl.DataFrame, candidatos: list[str]) -> str | None:
    cols = {c.lower(): c for c in df.columns}
    for cand in candidatos:
        cand = cand.lower()
        if cand in cols:
            return cols[cand]
        for low, orig in cols.items():
            if cand in low:
                return orig
    return None


def safe_pick(df: pl.DataFrame, preferido: str, extras: list[str]) -> str | None:
    if preferido in df.columns:
        return preferido
    return detectar_coluna(df, extras)


def limpar_pedidos(df: pl.DataFrame, coluna: str) -> pl.DataFrame:
    if coluna in df.columns:
        df = df.with_columns(pl.col(coluna).cast(pl.Utf8).str.strip_chars())
    return df


def ler_planilhas(pasta: str, nome: str) -> pl.DataFrame:
    if not os.path.exists(pasta):
        logging.error(f"‚ùå Pasta '{pasta}' n√£o existe.")
        return pl.DataFrame()

    arquivos = [a for a in os.listdir(pasta)
                if a.lower().endswith((".xls", ".xlsx"))
                and not a.startswith("~$")]

    if not arquivos:
        logging.warning(f"‚ö†Ô∏è Nenhum arquivo .xls ou .xlsx encontrado na pasta '{pasta}'.")
        return pl.DataFrame()

    logging.info(f"üìÇ {len(arquivos)} arquivos encontrados em '{nome}' para leitura.")

    dfs = []
    for arq in arquivos:
        path = os.path.join(pasta, arq)
        try:
            raw = pl.read_excel(path)
            df = next(iter(raw.values())) if isinstance(raw, dict) else raw
            dfs.append(df)
            logging.info(f"   ‚úîÔ∏è Arquivo '{arq}' lido com sucesso ({df.height} linhas).")
        except Exception as e:
            logging.error(f"   ‚ùå Erro ao ler o arquivo '{arq}': {e}")

    if not dfs:
        logging.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel ler nenhum arquivo da pasta '{pasta}'.")
        return pl.DataFrame()

    return pl.concat(dfs, how="diagonal_relaxed")


def salvar_resultado(df: pl.DataFrame, pasta: str, nome: str, limit: int):
    os.makedirs(pasta, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    ext = "csv" if df.height >= limit else "xlsx"
    caminho = os.path.join(pasta, f"{nome}_{ts}.{ext}")

    if ext == "csv":
        df.write_csv(caminho)
    else:
        df.write_excel(caminho)

    logging.info(f"üíæ Resultado salvo em: {caminho}")
    return caminho


def limpar_nome(nome: str) -> str:
    if not nome:
        return ""
    nome = str(nome).upper().strip()

    nome = re.sub(r"[^\x00-\x7F]+", "", nome)
    nome = re.sub(r"[-_]+", " ", nome)
    nome = re.sub(r"\s+", " ", nome)

    partes = nome.split(" ")

    if len(partes) == 2 and len(partes[0]) == 3 and len(partes[1]) == 2:
        return f"{partes[1]} {partes[0]}"

    return nome.strip()


def salvar_relatorio_intermediario(df: pl.DataFrame, nome: str, config: dict):
    """Fun√ß√£o auxiliar para salvar os DataFrames intermedi√°rios."""
    pasta_saida = config["caminhos"]["pasta_saida"]
    os.makedirs(pasta_saida, exist_ok=True)
    caminho = os.path.join(pasta_saida, f"{nome}.parquet")
    df.write_parquet(caminho)
    logging.info(f"üìÑ Relat√≥rio intermedi√°rio salvo: {caminho}")


class AnaliseRetidos:
    # üî• ALTERA√á√ÉO AQUI: Mudar o arquivo de configura√ß√£o padr√£o
    def __init__(self, config_filename="config_2.json"):
        base = os.path.dirname(os.path.abspath(__file__))
        path = os.path.join(base, config_filename)

        if not os.path.exists(path):
            logging.error(f"‚ùå Arquivo de configura√ß√£o n√£o encontrado em: {path}")
            raise FileNotFoundError(f"Arquivo de configura√ß√£o n√£o encontrado: {path}")

        with open(path, "r", encoding="utf-8") as f:
            self.config = json.load(f)

        self.removidos = {"cluster": 0, "devolucao": 0, "problematicos": 0, "custodia": 0}
        self.total_inicial_filtrado = 0
        self.df_total_por_base = pl.DataFrame()

    def executar(self):
        logging.info("üöÄ Iniciando an√°lise de pacotes retidos...")

        df = self._processar_dados()

        if df.is_empty():
            logging.error("‚ùå Nenhum dado final encontrado ap√≥s o processamento. O script ser√° encerrado.")
            return

        logging.info(f"üìä DataFrame final possui {df.height} linhas e {len(df.columns)} colunas.")

        df = self._enriquecer_com_coordenadores(df)

        if "Coordenador" not in df.columns:
            logging.error(
                "‚ùå A coluna 'Coordenador' n√£o foi encontrada no DataFrame ap√≥s o enriquecimento. N√£o √© poss√≠vel enviar os cards.")
            return

        coordenadores_encontrados = df.filter(pl.col("Coordenador").is_not_null())["Coordenador"].unique().to_list()
        if not coordenadores_encontrados:
            logging.error("‚ùå Nenhum coordenador foi encontrado/enriquecido nos dados. N√£o h√° cards para enviar.")
            return

        logging.info(f"üìä Coordenadores encontrados para envio: {len(coordenadores_encontrados)}")

        df_anterior = self._carregar_snapshot_anterior()
        if df_anterior.is_empty():
            logging.info("üìÇ Nenhum snapshot do dia anterior encontrado. A varia√ß√£o ser√° zero.")
        else:
            logging.info(f"üìÇ Snapshot do dia anterior carregado com {df_anterior.height} linhas.")

        self._gerar_log_comparativo(df, df_anterior)

        self._enviar_card_completo(df, df_anterior)

        self._salvar_snapshot_diario(df)

        caminho_final = self._salvar_resultado_final(df)
        self._exibir_resumo_console(caminho_final)

    # ============================================================
    # üîß PROCESSO PRINCIPAL ‚Äî PIPELINE COMPLETO
    # ============================================================
    def _processar_dados(self):
        df = self._ler_e_preparar_retidos()
        if df.is_empty():
            logging.error("‚ùå Nenhum dado retido inicial foi lido. Abortando processamento.")
            return pl.DataFrame()

        logging.info(f"üìä Ap√≥s leitura inicial: {df.height} pacotes retidos.")
        salvar_relatorio_intermediario(df, "00_Retidos_Iniciais", self.config)

        df = self._aplicar_filtro_devolucao(df)
        logging.info(f"üìä Ap√≥s filtro de devolu√ß√£o: {df.height} pacotes restantes.")

        df = self._aplicar_filtro_problematicos(df)
        logging.info(f"üìä Ap√≥s filtro de problem√°ticos: {df.height} pacotes restantes.")

        df = self._aplicar_filtro_custodia(df)
        logging.info(f"üìä Ap√≥s filtro de cust√≥dia: {df.height} pacotes restantes (FINAL).")

        return df

    # ============================================================
    # üì• Leitura + organiza√ß√£o dos retidos
    # ============================================================
    def _ler_e_preparar_retidos(self):
        df = ler_planilhas(self.config["caminhos"]["pasta_retidos"], "Retidos")
        if df.is_empty():
            return pl.DataFrame()

        col_dias = safe_pick(df, "Dias Retidos ÊªûÁïôÊó•", ["ÊªûÁïô", "dias"])
        if col_dias:
            antes = df.height
            df = df.with_columns(pl.col(col_dias).cast(pl.Int64, strict=False))
            df = df.filter(pl.col(col_dias) > 6)
            self.removidos["cluster"] = antes - df.height
            logging.info(f"üîµ Filtro de >6 dias aplicado. Removidos: {self.removidos['cluster']}.")

        col_base = safe_pick(df, "Base de Entrega Ê¥æ‰ª∂ÁΩëÁÇπ", ["base", "ÁΩëÁÇπ"])
        if col_base:
            self.df_total_por_base = (
                df.with_columns(
                    pl.col(col_base)
                    .map_elements(limpar_nome, return_dtype=pl.Utf8)
                    .alias("Base_Clean")
                )
                .group_by("Base_Clean")
                .agg(pl.len().alias("Total de Pedidos"))
            )

        col_pedido = safe_pick(df, self.config["colunas"]["col_pedido_ret"], ["pedido", "ËøêÂçï"])
        col_data = safe_pick(df, self.config["colunas"]["col_data_atualizacao_ret"], ["data", "Êõ¥Êñ∞"])
        col_regional = safe_pick(df, self.config["colunas"]["col_regional_ret"], ["regional", "Âå∫Âüü"])

        if not all([col_pedido, col_data, col_regional, col_base]):
            logging.error(
                "‚ùå Uma ou mais colunas essenciais n√£o foram encontradas na planilha de Retidos. Verifique o config.json e os nomes das colunas.")
            return pl.DataFrame()

        df = df.select([col_pedido, col_data, col_regional, col_base]).rename({
            col_pedido: self.config["colunas"]["col_pedido_ret"],
            col_data: self.config["colunas"]["col_data_atualizacao_ret"],
            col_regional: self.config["colunas"]["col_regional_ret"],
            col_base: "Base de Entrega Ê¥æ‰ª∂ÁΩëÁÇπ"
        })

        df = limpar_pedidos(df, self.config["colunas"]["col_pedido_ret"])
        df = converter_datetime(df, self.config["colunas"]["col_data_atualizacao_ret"])

        df = df.filter(
            pl.col(self.config["colunas"]["col_regional_ret"])
            .is_in(pl.Series(self.config["parametros"]["regionais_desejadas"]))
        )

        self.total_inicial_filtrado = df.height
        return df

    # ============================================================
    # üîµ Filtro: Devolu√ß√£o
    # ============================================================
    def _aplicar_filtro_devolucao(self, df):
        df_dev = ler_planilhas(self.config["caminhos"]["pasta_devolucao"], "Devolu√ß√£o")
        if df_dev.is_empty():
            logging.warning("‚ö†Ô∏è Planilha de devolu√ß√£o n√£o encontrada ou vazia. Pulando este filtro.")
            return df

        col_pedido = safe_pick(df_dev, self.config["colunas"]["col_pedido_dev"], ["pedido"])
        col_data = safe_pick(df_dev, self.config["colunas"]["col_data_solicitacao_dev"], ["tempo", "solic"])

        if not col_pedido or not col_data:
            logging.warning(
                "‚ö†Ô∏è Colunas de pedido ou data n√£o encontradas na planilha de devolu√ß√£o. Pulando este filtro.")
            return df

        df_dev = (
            df_dev.select([col_pedido, col_data])
            .rename({
                col_pedido: self.config["colunas"]["col_pedido_dev"],
                col_data: self.config["colunas"]["col_data_solicitacao_dev"]
            })
            .pipe(limpar_pedidos, self.config["colunas"]["col_pedido_dev"])
            .pipe(converter_datetime, self.config["colunas"]["col_data_solicitacao_dev"])
            .group_by(self.config["colunas"]["col_pedido_dev"])
            .agg(pl.col(self.config["colunas"]["col_data_solicitacao_dev"]).min())
        )

        dfj = df.join(
            df_dev,
            left_on=self.config["colunas"]["col_pedido_ret"],
            right_on=self.config["colunas"]["col_pedido_dev"],
            how="left"
        )

        df_rem = dfj.filter(
            (pl.col(self.config["colunas"]["col_data_solicitacao_dev"])
             > pl.col(self.config["colunas"]["col_data_atualizacao_ret"]))
            &
            pl.col(self.config["colunas"]["col_data_solicitacao_dev"]).is_not_null()
        )

        salvar_relatorio_intermediario(df_rem, "01_Removidos_Devolucao", self.config)

        remover = df_rem.select(self.config["colunas"]["col_pedido_ret"]).to_series()
        self.removidos["devolucao"] = remover.len()
        logging.info(f"üîµ Filtro de devolu√ß√£o aplicado. Removidos: {self.removidos['devolucao']}.")

        return df.filter(~pl.col(self.config["colunas"]["col_pedido_ret"]).is_in(remover))

    # ============================================================
    # üü£ Filtro: Problem√°ticos
    # ============================================================
    def _aplicar_filtro_problematicos(self, df):
        df_prob = ler_planilhas(self.config["caminhos"]["pasta_problematicos"], "Problem√°ticos")
        if df_prob.is_empty():
            logging.warning("‚ö†Ô∏è Planilha de problem√°ticos n√£o encontrada ou vazia. Pulando este filtro.")
            return df

        col_pedido = safe_pick(df_prob, "N√∫mero de pedido JMS", ["pedido", "ËøêÂçï"])
        col_data = safe_pick(df_prob, "data de registro", ["registro", "ÂºÇÂ∏∏"])

        if not col_pedido or not col_data:
            logging.warning(
                "‚ö†Ô∏è Colunas de pedido ou data n√£o encontradas na planilha de problem√°ticos. Pulando este filtro.")
            return df

        df_prob = (
            df_prob.select([col_pedido, col_data])
            .rename({
                col_pedido: "Pedido_Prob",
                col_data: "Registro_Prob"
            })
            .pipe(limpar_pedidos, "Pedido_Prob")
            .pipe(converter_datetime, "Registro_Prob")
            .group_by("Pedido_Prob")
            .agg(pl.col("Registro_Prob").min())
        )

        dfj = df.join(
            df_prob,
            left_on=self.config["colunas"]["col_pedido_ret"],
            right_on="Pedido_Prob",
            how="left"
        )

        df_rem = dfj.filter(
            pl.col("Registro_Prob") >= pl.col(self.config["colunas"]["col_data_atualizacao_ret"])
        )

        salvar_relatorio_intermediario(df_rem, "02_Removidos_Problematicos", self.config)

        remover = df_rem.select(self.config["colunas"]["col_pedido_ret"]).to_series()
        self.removidos["problematicos"] = remover.len()
        logging.info(f"üü£ Filtro de problem√°ticos aplicado. Removidos: {self.removidos['problematicos']}.")

        return df.filter(~pl.col(self.config["colunas"]["col_pedido_ret"]).is_in(remover))

    # ============================================================
    # üü¶ Filtro: Cust√≥dia
    # ============================================================
    def _aplicar_filtro_custodia(self, df):
        df_cust = ler_planilhas(self.config["caminhos"]["pasta_custodia"], "Cust√≥dia")
        if df_cust.is_empty():
            logging.warning("‚ö†Ô∏è Planilha de cust√≥dia n√£o encontrada ou vazia. Pulando este filtro.")
            return df

        col_pedido = safe_pick(df_cust, self.config["colunas"]["col_pedido_cust"], ["pedido"])
        col_data = safe_pick(df_cust, self.config["colunas"]["col_data_registro_cust"], ["registro"])

        if not col_pedido or not col_data:
            logging.warning(
                "‚ö†Ô∏è Colunas de pedido ou data n√£o encontradas na planilha de cust√≥dia. Pulando este filtro.")
            return df

        df_cust = (
            df_cust.select([col_pedido, col_data])
            .rename({
                col_pedido: self.config["colunas"]["col_pedido_cust"],
                col_data: self.config["colunas"]["col_data_registro_cust"]
            })
            .pipe(limpar_pedidos, self.config["colunas"]["col_pedido_cust"])
            .pipe(converter_datetime, self.config["colunas"]["col_data_registro_cust"])
            .group_by(self.config["colunas"]["col_pedido_cust"])
            .agg(pl.col(self.config["colunas"]["col_data_registro_cust"]).min())
            .with_columns(
                (
                        pl.col(self.config["colunas"]["col_data_registro_cust"])
                        + pl.duration(days=self.config["parametros"]["prazo_custodia_dias"])
                ).alias("Prazo_Limite")
            )
        )

        dfj = df.join(
            df_cust,
            left_on=self.config["colunas"]["col_pedido_ret"],
            right_on=self.config["colunas"]["col_pedido_cust"],
            how="left"
        )

        col_data_atual = self.config["colunas"]["col_data_atualizacao_ret"]

        dfj = dfj.with_columns([
            pl.when(
                (pl.col(col_data_atual) <= pl.col("Prazo_Limite"))
                & pl.col("Prazo_Limite").is_not_null()
            )
            .then(pl.lit("Dentro"))
            .otherwise(pl.lit("Fora"))
            .alias("Status_Custodia")
        ])

        df_rem = dfj.filter(pl.col("Status_Custodia") == "Dentro")

        salvar_relatorio_intermediario(df_rem, "03_Removidos_Custodia", self.config)

        self.removidos["custodia"] = df_rem.height
        logging.info(f"üü¶ Filtro de cust√≥dia aplicado. Removidos: {self.removidos['custodia']}.")

        return dfj.filter(pl.col("Status_Custodia") == "Fora").drop("Status_Custodia", "Prazo_Limite")

    # ============================================================
    # ü§ù Coordenadores (Com Normaliza√ß√£o)
    # ============================================================
    def _enriquecer_com_coordenadores(self, df):
        path = self.config["caminhos"]["caminho_coordenador"]
        if not os.path.exists(path):
            logging.warning(f"‚ö†Ô∏è Planilha de coordenadores n√£o encontrada em: {path}")
            return df

        try:
            raw = pl.read_excel(path)
            dfc = next(iter(raw.values())) if isinstance(raw, dict) else raw

            logging.info(f"üìÇ Lendo planilha de coordenadores. Colunas encontradas: {dfc.columns}")

            col_base = detectar_coluna(dfc, ["base", "nome da base", "entrega", "Ê¥æ‰ª∂ÁΩëÁÇπ"])
            col_coord = detectar_coluna(dfc, ["coordenador", "respons√°vel", "Ë¥üË¥£‰∫∫"])

            logging.info(f"üìÇ Coluna de Base detectada: '{col_base}' | Coluna de Coordenador detectada: '{col_coord}'")

            if not col_base or not col_coord:
                logging.warning(
                    "‚ö†Ô∏è Colunas de 'base' ou 'coordenador' n√£o foram detectadas na planilha. Verifique os nomes.")
                return df

            dfc = dfc.with_columns([
                pl.col(col_base).map_elements(limpar_nome, return_dtype=pl.Utf8).alias("Base_Coord"),
                pl.col(col_coord).alias("Coordenador")
            ]).select(["Base_Coord", "Coordenador"])

            df = df.with_columns(
                pl.col("Base de Entrega Ê¥æ‰ª∂ÁΩëÁÇπ")
                .map_elements(limpar_nome, return_dtype=pl.Utf8)
                .alias("Base_Normalizada")
            )

            df_final = df.join(
                dfc,
                left_on="Base_Normalizada",
                right_on="Base_Coord",
                how="left"
            )

            nulos_apos_join = df_final.filter(pl.col("Coordenador").is_null()).height
            if nulos_apos_join > 0:
                logging.warning(
                    f"‚ö†Ô∏è Ap√≥s o join, {nulos_apos_join} bases n√£o encontraram um coordenador correspondente.")

            return df_final

        except Exception as e:
            logging.error(f"‚ùå Erro ao processar planilha de coordenadores: {e}")
            return df

    # ============================================================
    # üÜï RELAT√ìRIO DE COMPARA√á√ÉO (NOVO)
    # ============================================================
    def _gerar_log_comparativo(self, df_atual: pl.DataFrame, df_anterior: pl.DataFrame):
        logging.info("üìà Gerando relat√≥rio de compara√ß√£o com o dia anterior...")

        total_atual = df_atual.height
        total_anterior = df_anterior.height if not df_anterior.is_empty() else 0
        diff_total = total_atual - total_anterior

        atual_group = df_atual.group_by("Base_Normalizada").agg(pl.len().alias("Qtd_Atual"))

        if df_anterior.is_empty():
            ant_group = pl.DataFrame(schema={"Base_Normalizada": pl.Utf8, "Qtd_Anterior": pl.Int64})
        else:
            ant_group = df_anterior.group_by("Base_Normalizada").agg(pl.len().alias("Qtd_Anterior"))

        comparacao_bases = (
            atual_group.join(ant_group, on="Base_Normalizada", how="left")
            .with_columns([
                pl.col("Qtd_Anterior").fill_null(0),
                (pl.col("Qtd_Atual") - pl.col("Qtd_Anterior")).alias("Variacao")
            ])
        )

        top_aumentos = comparacao_bases.filter(pl.col("Variacao") > 0).sort("Variacao", descending=True).head(5)
        top_reducoes = comparacao_bases.filter(pl.col("Variacao") < 0).sort("Variacao").head(5)

        texto_log = [
            "=" * 50,
            f"üìä RELAT√ìRIO COMPARATIVO - {datetime.now():%d/%m/%Y}",
            "=" * 50,
            "",
            "üìà **RESUMO GERAL:**",
            f"  - Total de Retidos (Hoje): {total_atual}",
            f"  - Total de Retidos (Ontem): {total_anterior}",
            f"  - Varia√ß√£o Geral: {'+' if diff_total >= 0 else ''}{diff_total} ({'Aumento' if diff_total > 0 else 'Redu√ß√£o' if diff_total < 0 else 'Est√°vel'})",
            "",
            "üî¥ **TOP 5 BASES COM MAIOR AUMENTO:**",
        ]

        if top_aumentos.is_empty():
            texto_log.append("  - Nenhuma base apresentou aumento.")
        else:
            for row in top_aumentos.iter_rows(named=True):
                texto_log.append(
                    f"  - {row['Base_Normalizada']}: +{row['Variacao']} (De {row['Qtd_Anterior']} para {row['Qtd_Atual']})")

        texto_log.extend([
            "",
            "üü¢ **TOP 5 BASES COM MAIOR REDU√á√ÉO:**",
        ])

        if top_reducoes.is_empty():
            texto_log.append("  - Nenhuma base apresentou redu√ß√£o.")
        else:
            for row in top_reducoes.iter_rows(named=True):
                texto_log.append(
                    f"  - {row['Base_Normalizada']}: {row['Variacao']} (De {row['Qtd_Anterior']} para {row['Qtd_Atual']})")

        texto_log.append("=" * 50)

        pasta_saida = self.config["caminhos"]["pasta_saida"]
        os.makedirs(pasta_saida, exist_ok=True)
        caminho_log = os.path.join(pasta_saida, f"log_comparativo_{datetime.now():%Y%m%d}.log")

        with open(caminho_log, "w", encoding="utf-8") as f:
            f.write("\n".join(texto_log))

        logging.info(f"üìÑ Relat√≥rio comparativo salvo em: {caminho_log}")

        logging.info("\n" + "\n".join(texto_log))

    # ============================================================
    # üíæ Salvamento final
    # ============================================================
    def _salvar_resultado_final(self, df):
        return salvar_resultado(
            df,
            self.config["caminhos"]["pasta_saida"],
            self.config["parametros"]["nome_arquivo_final"],
            self.config["parametros"]["excel_row_limit"]
        )

    # ============================================================
    # üìü Resumo no console
    # ============================================================
    def _exibir_resumo_console(self, caminho):
        logging.info("=============== RESUMO FINAL ===============")
        logging.info(f"Retidos iniciais (filtrados por regional e >6 dias): {self.total_inicial_filtrado}")
        logging.info(f"Removidos - Cluster (>6 dias): {self.removidos['cluster']}")
        logging.info(f"Removidos - Devolu√ß√£o: {self.removidos['devolucao']}")
        logging.info(f"Removidos - Problem√°ticos: {self.removidos['problematicos']}")
        logging.info(f"Removidos - Cust√≥dia (no prazo): {self.removidos['custodia']}")
        total_final = self.total_inicial_filtrado - sum(self.removidos.values())
        logging.info(f"Total de pacotes retidos para a√ß√£o: {total_final}")
        logging.info(f"Arquivo final salvo em: {caminho}")
        logging.info("===========================================")

    # ============================================================
    # üíæ Salvar snapshot do dia
    # ============================================================
    def _salvar_snapshot_diario(self, df: pl.DataFrame):
        pasta = os.path.join(self.config["caminhos"]["pasta_saida"], "Snapshots")
        os.makedirs(pasta, exist_ok=True)

        hoje = datetime.now().strftime("%Y%m%d")
        caminho = os.path.join(pasta, f"retidos_{hoje}.parquet")

        df.write_parquet(caminho)
        logging.info(f"üì¶ Snapshot di√°rio salvo em: {caminho}")
        return caminho

    # ============================================================
    # üìÇ Carregar snapshot anterior (VERS√ÉO MELHORADA E MAIS CONFI√ÅVEL)
    # ============================================================
    def _carregar_snapshot_anterior(self) -> pl.DataFrame:
        pasta = os.path.join(self.config["caminhos"]["pasta_saida"], "Snapshots")

        if not os.path.exists(pasta):
            logging.info("üìÇ Pasta de Snapshots n√£o existe. Nenhum dado anterior para comparar.")
            return pl.DataFrame()

        # CORRE√á√ÉO 2: Usar timedelta do Python para calcular a data de ontem
        ontem = datetime.now() - timedelta(days=1)
        nome_arquivo_anterior = ontem.strftime("retidos_%Y%m%d.parquet")
        caminho_anterior = os.path.join(pasta, nome_arquivo_anterior)

        if os.path.exists(caminho_anterior):
            try:
                df = pl.read_parquet(caminho_anterior)
                logging.info(f"üìÇ Snapshot do dia anterior ('{nome_arquivo_anterior}') carregado com sucesso.")
                return df
            except Exception as e:
                logging.error(f"‚ùå Falha ao ler o snapshot do dia anterior '{nome_arquivo_anterior}': {e}")
                return pl.DataFrame()
        else:
            logging.warning(
                f"‚ö†Ô∏è Snapshot do dia anterior ('{nome_arquivo_anterior}') n√£o foi encontrado. A compara√ß√£o n√£o ser√° feita.")
            return pl.DataFrame()

    # ============================================================
    # üìÆ CARD COMPLETO POR COORDENADOR (Webhook fixo) - VERS√ÉO CORRIGIDA
    # ============================================================
    def _enviar_card_completo(self, df_atual: pl.DataFrame, df_anterior: pl.DataFrame):
        WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/b8328e19-9b9f-40d5-bce0-6af7f4612f1b"

        logging.info("üìÆ Iniciando a montagem e envio dos cards para os coordenadores...")

        atual_group = (
            df_atual.group_by(["Coordenador", "Base_Normalizada"])
            .agg(pl.len().alias("Qtd_Atual"))
        )

        if df_anterior.is_empty():
            ant_group = pl.DataFrame(
                schema={"Coordenador": pl.Utf8, "Base_Normalizada": pl.Utf8, "Qtd_Anterior": pl.Int64})
        else:
            ant_group = (
                df_anterior.group_by(["Coordenador", "Base_Normalizada"])
                .agg(pl.len().alias("Qtd_Anterior"))
            )

        resumo = (
            atual_group.join(
                ant_group,
                on=["Coordenador", "Base_Normalizada"],
                how="left"
            )
            .with_columns(
                pl.col("Qtd_Anterior").fill_null(0)
            )
            .with_columns(
                (pl.col("Qtd_Atual") - pl.col("Qtd_Anterior")).alias("Variacao")
            )
        )

        coordenadores = resumo.filter(pl.col("Coordenador").is_not_null())["Coordenador"].unique().to_list()
        logging.info(f"üìÆ Total de coordenadores √∫nicos a processar: {len(coordenadores)}")

        for coord in coordenadores:
            if not coord:
                continue

            dfc = resumo.filter(pl.col("Coordenador") == coord)
            total_atual = dfc["Qtd_Atual"].sum()
            total_anterior = dfc["Qtd_Anterior"].sum()
            diff_total = total_atual - total_anterior

            top3 = dfc.sort("Qtd_Atual", descending=True).head(3)

            linhas = []
            for row in top3.iter_rows(named=True):
                base = row["Base_Normalizada"]
                qtd = row["Qtd_Atual"]
                var = row["Variacao"]

                seta = "üî∫" if var > 0 else "üü¢" if var < 0 else "‚ö™"
                legenda = f"aumentou {var}" if var > 0 else f"reduziu {abs(var)}" if var < 0 else "sem mudan√ßa"
                linhas.append(f"- {seta} **{base}**: **{qtd} pedidos** ({legenda})")

            texto = (
                    f"üìÖ **Data de Gera√ß√£o:**\n{datetime.now():%d/%m/%Y %H:%M}\n\n"
                    f"üì¶ **Qtd de Pacotes:** {total_atual}\n"
                    f"üìä **Varia√ß√£o de Pacotes:** "
                    f"{'üìà Aumentou' if diff_total > 0 else 'üìâ Reduziu' if diff_total < 0 else '‚ûñ Igual'} "
                    f"{abs(diff_total)} pedidos\n\n"
                    f"üî¥ **3 Piores Bases:**\n" + "\n".join(linhas)
            )

            card = {
                "msg_type": "interactive",
                "card": {
                    "header": {
                        "title": {"tag": "plain_text", "content": f"Retidos ‚Äì {coord}"},
                        "template": "red"
                    },
                    "elements": [
                        {
                            "tag": "div",
                            "text": {
                                "tag": "lark_md",
                                "content": texto
                            }
                        }
                    ]
                }
            }

            logging.info(f"üìÆ Enviando card para o coordenador: {coord}...")

            try:
                response = requests.post(WEBHOOK, json=card, timeout=10)
                logging.info(
                    f"üìÆ Resposta do webhook para '{coord}': Status {response.status_code} - Conte√∫do: {response.text}")

                if response.status_code == 200:
                    logging.info(f"‚úÖ Card enviado com sucesso para o coordenador: {coord}")
                else:
                    logging.error(f"‚ùå Falha ao enviar card para '{coord}'. Status Code: {response.status_code}")
            except Exception as e:
                logging.error(f"‚ùå Erro na requisi√ß√£o de envio do card para o coordenador '{coord}': {e}")


# ============================================================
# üèÅ PONTO DE PARTIDA
# ============================================================
if __name__ == "__main__":
    setup_logging()
    try:
        analisador = AnaliseRetidos()
        analisador.executar()
    except Exception as e:
        logging.critical(f"üí• Ocorreu um erro cr√≠tico na execu√ß√£o principal: {e}", exc_info=True)