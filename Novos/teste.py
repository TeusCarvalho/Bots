import pandas as pd
import os
import numpy as np
from tqdm import tqdm
from datetime import datetime
import shutil
import logging
from typing import List, Dict, Optional, Any

# ==============================================================================
# --- CONFIGURA√á√ÉO GERAL ---
# Todas as configura√ß√µes dos scripts foram unificadas aqui para f√°cil acesso.
# ==============================================================================

# --- 1. Caminhos Principais ---
# Altere estes caminhos para corresponder √† estrutura de pastas do seu ambiente.
BASE_PATH = r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda (1)\√Årea de Trabalho\Testes\Sem Movimenta√ß√£o'
OUTPUT_BASE_PATH = r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda\Jt - Relat√≥rios'
COORDENADOR_BASE_PATH = r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda (1)\√Årea de Trabalho\Testes\Coordenador'

# --- 2. Pastas e Arquivos de Entrada ---
PATH_INPUT_MAIN = os.path.join(BASE_PATH, 'Sem_Movimenta√ß√£o')
PATH_INPUT_PROBLEMATICOS = os.path.join(BASE_PATH, 'Pacotes Problematicos')
PATH_INPUT_DEVOLUCAO = os.path.join(BASE_PATH, 'Devolu√ß√£o')
ARQUIVO_MAPEAMENTO_COORDENADORES = os.path.join(COORDENADOR_BASE_PATH, 'Base_Atualizada.xlsx')

# --- 3. Pastas de Sa√≠da ---
PATH_OUTPUT_REPORTS = OUTPUT_BASE_PATH
PATH_OUTPUT_ARQUIVO_MORTO = os.path.join(OUTPUT_BASE_PATH, "Arquivo Morto")

# --- 4. Nomes de Arquivos e Colunas ---
FILENAME_START_MAIN = 'Monitoramento de movimenta√ß√£o em tempo real'

# Colunas principais
COL_REMESSA = 'Remessa'
COL_DIAS_PARADO = 'Dias Parado'
COL_ULTIMA_OPERACAO = 'Tipo da √∫ltima opera√ß√£o'
COL_REGIONAL = 'Regional respons√°vel'
COL_NOME_PROBLEMATICO = 'Nome de pacote problem√°tico'
# ALTERADO: De 'Hor√°rio da √∫ltima opera√ß√£o' para 'Aging'
COL_HORA_OPERACAO = 'Aging'  # ALTERADO: Usando a coluna 'Aging' em vez de 'Hor√°rio da √∫ltima opera√ß√£o'
COL_DEVOLUCAO = 'Devolu√ß√£o'
COL_STATUS = 'Status'
COL_MULTA = 'Multa (R$)'
COL_BASE_RECENTE = 'Nome da base mais recente'
COL_TRANSITO = 'Tr√¢nsito'

# Colunas para mapeamento de coordenadores
COLUNA_CHAVE_PRINCIPAL = 'Unidade respons√°vel'
COLUNA_CHAVE_MAPEAMENTO = 'Nome da base'
COLUNA_INFO_COORDENADOR = 'Coordenadores'
COLUNA_INFO_FILIAL = 'Filial'
NOVA_COLUNA_COORDENADOR = 'Coordenadores'
NOVA_COLUNA_FILIAL = 'Filial'

# --- 5. Listas para Regras de Neg√≥cio ---
FRANQUIAS = ["F AGL-GO", "F ALV-AM", "F APG - GO", "F ARQ - RO", "F BSB-DF", "F CDN-AM", "F CGR - MS", "F FMA-GO",
             "F GYN - GO", "F ITI-PA", "F RVD - GO", "F TRD-GO", "F CGR 02-MS", "F GYN 02-GO", "F OCD-GO", "F PVH-RO",
             "F TGT-DF", "F DOM -PA", "F JCD-PA", "F MCP-AP", "F ORL-PA", "F PCA-PA", "F RDC -PA", "F SFX-PA",
             "F TLA-PA"]
UNIDADES_SC_DC = ["DC AGB-MT", "DC CGR-MS", "DC GYN-GO", "DC JUI-MT", "DC PVH-RO", "DC RBR-AC", "DF BSB", "GYN -GO",
                  "MT CGB"]
BASES_FLUXO_INVERSO = ["VLP -GO", "VHL-RO", "VGR-MT", "VGR 02-MT", "URC -GO", "TRD -GO", "TLL -MS", "TGT -DF",
                       "TGA -MT", "TAR -AC", "SRS -MT", "SNP -MT", "SMD-AC", "SMB -GO", "SMA -GO", "SJA -GO",
                       "SGO -MS", "SEN-GO", "SBN -DF", "SAMS -DF", "SAD -GO", "RVD -GO", "ROO -MT", "RFI -DF",
                       "RDM -RO", "RBR -AC", "RBR 02-AC", "QUI -GO", "QRN -MT", "PVL -MT", "PVH -RO", "PVH 02-RO",
                       "PTD -MT", "PRG -GO", "PRB -MS", "POS -GO", "PON -GO", "PNT-MS", "PLN -GO", "PLDF -DF",
                       "PA GYN-GO", "OCD-GO", "NVT -MT", "NVR -MS", "NDI -MS", "MT CGB", "MDT -MT", "LUZ -GO",
                       "LRV -MT", "JUI -MT", "JTI -GO", "JRD -MS", "JPN -RO", "ITR -GO", "IPR -GO", "GYN -GO",
                       "GYN 07-GO", "GYN 06-GO", "GYN 05-GO", "GYN 04-GO", "GYN 03-GO", "GYN 02-GO", "GUA -DF", "GP",
                       "GNT -MT", "GNA -GO", "GAM -DF", "FMA -GO", "FAI -GO", "F TRD-GO", "F RVD - GO", "F OCD - GO",
                       "F GYN - GO", "F FMA-GO", "F CGR - MS", "F BSB-DF", "F BSB - DF", "F APG - GO", "F AGL-GO",
                       "EMA -DF", "DOU -MS", "CZS -AC", "CXM -MS", "CTN -GO", "CTL -GO", "CRB -MS", "CNF -MT", "CMP-MT",
                       "CHS -MS", "CGR -MS", "CGR 05-MS", "CGR 04-MS", "CGR 03-MS", "CGR 02-MS", "CGB 05-MT",
                       "CGB 04-MT", "CGB 03-MT", "CGB 02-MT", "CEIS -DF", "CEIN -DF", "CCR -MT", "CAPI -GO", "CAN -GO",
                       "BSB -DF", "BGA -MT", "ATF -MT", "ARQ -RO", "ARI -MT", "AQD -MS", "APG -GO", "ANP -GO",
                       "AMB -MS", "AGL -GO", "AGB -MT"]
DESTINOS_FLUXO_INVERSO = ["MAO -AM", "DC AGB-MT", "DC CGR-MS", "DC GYN-GO", "DC JUI-MT", "DC MAO-AM", "DC MRB-PA",
                          "DC PMW-TO", "DC PVH-RO", "DC RBR-AC", "DC STM-PA", "DF BSB"]
BASES_CD = BASES_FLUXO_INVERSO

# --- 6. Configura√ß√£o do Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)


# ==============================================================================
# --- FUN√á√ïES DE CARREGAMENTO DE DADOS ---
# ==============================================================================

def encontrar_arquivo_principal(pasta: str, inicio_nome: str) -> Optional[str]:
    """Busca por um arquivo Excel em uma pasta que comece com um determinado nome."""
    try:
        for nome_arquivo in os.listdir(pasta):
            if nome_arquivo.startswith(inicio_nome) and nome_arquivo.endswith(('.xlsx', '.xls')):
                logging.info(f"Arquivo principal encontrado: {nome_arquivo}")
                return os.path.join(pasta, nome_arquivo)
    except FileNotFoundError:
        logging.error(f"A pasta de leitura '{pasta}' n√£o foi encontrada.")
        return None
    logging.warning(f"Nenhum arquivo come√ßando com '{inicio_nome}' foi encontrado em '{pasta}'.")
    return None


def carregar_planilhas_de_pasta(caminho_pasta: str, descricao_tqdm: str) -> pd.DataFrame:
    """L√™ todos os arquivos Excel de uma pasta e consolida seus dados em um √∫nico DataFrame."""
    lista_dfs = []
    nome_pasta = os.path.basename(caminho_pasta)
    logging.info(f"Lendo planilhas da pasta: {nome_pasta}")
    try:
        arquivos = [f for f in os.listdir(caminho_pasta) if f.endswith(('.xlsx', '.xls'))]
        if not arquivos:
            logging.warning(f"Nenhum arquivo Excel encontrado na pasta '{nome_pasta}'.")
            return pd.DataFrame()
        for arquivo in tqdm(arquivos, desc=descricao_tqdm):
            caminho_completo = os.path.join(caminho_pasta, arquivo)
            try:
                abas = pd.read_excel(caminho_completo, sheet_name=None)
                lista_dfs.extend(abas.values())
            except Exception as e:
                logging.error(f"Falha ao ler o arquivo '{arquivo}' da pasta '{nome_pasta}': {e}")
                continue
        if not lista_dfs:
            return pd.DataFrame()
        df_consolidado = pd.concat(lista_dfs, ignore_index=True)
        logging.info(f"Total de {len(df_consolidado)} registros consolidados de '{nome_pasta}'.")
        return df_consolidado
    except FileNotFoundError:
        logging.error(f"A pasta '{caminho_pasta}' n√£o foi encontrada. Processo interrompido.")
        raise
    except Exception as e:
        logging.error(f"Ocorreu um erro inesperado ao ler os arquivos da pasta '{nome_pasta}': {e}")
        raise


# ==============================================================================
# --- FUN√á√ïES DE PROCESSAMENTO E L√ìGICA DE NEG√ìCIO ---
# ==============================================================================

def aplicar_regras_transito(df: pd.DataFrame) -> pd.DataFrame:
    """Aplica regras de neg√≥cio para definir o status de 'Tr√¢nsito' dos pacotes."""
    logging.info("Aplicando regras de tr√¢nsito...")
    if COL_BASE_RECENTE not in df.columns:
        logging.warning(f"Coluna '{COL_BASE_RECENTE}' n√£o encontrada. As regras de tr√¢nsito n√£o ser√£o aplicadas.")
        df[COL_TRANSITO] = "COLUNA DE BASE RECENTE N√ÉO ENCONTRADA"
        return df

    cond_em_transito = df[COL_ULTIMA_OPERACAO] == "Âèë‰ª∂Êâ´Êèè/Bipe de expedi√ß√£o"
    is_fluxo_inverso = df[COL_BASE_RECENTE].isin(BASES_FLUXO_INVERSO) & df[COL_REGIONAL].isin(DESTINOS_FLUXO_INVERSO)
    origem_sc_bre = df[COL_BASE_RECENTE] == 'SC BRE'
    destino_pvh = df[COL_REGIONAL].str.contains('PVH-RO', na=False, case=False)

    prazo_fluxo_inverso_estourado = df[COL_DIAS_PARADO] >= 3
    prazo_5_dias_estourado = df[COL_DIAS_PARADO] >= 5
    prazo_3_dias_estourado = df[COL_DIAS_PARADO] >= 3

    conditions = [
        cond_em_transito & is_fluxo_inverso & prazo_fluxo_inverso_estourado,
        cond_em_transito & is_fluxo_inverso & ~prazo_fluxo_inverso_estourado,
        cond_em_transito & origem_sc_bre & prazo_5_dias_estourado,
        cond_em_transito & origem_sc_bre & ~prazo_5_dias_estourado,
        cond_em_transito & ~origem_sc_bre & destino_pvh & prazo_5_dias_estourado,
        cond_em_transito & ~origem_sc_bre & destino_pvh & ~prazo_5_dias_estourado,
        cond_em_transito & ~origem_sc_bre & ~destino_pvh & prazo_3_dias_estourado,
        cond_em_transito & ~origem_sc_bre & ~destino_pvh & ~prazo_3_dias_estourado,
    ]
    choices = [
        "VERIFICAR COM TRANSPORTE: VE√çCULO N√ÉO CHEGOU (FLUXO INVERSO)", "EM TR√ÇNSITO (FLUXO INVERSO)",
        "FALTA BIPE DE RECEBIMENTO (EXPEDIDO E N√ÉO CHEGOU)", "EM TR√ÇNSITO PARA A BASE",
        "FALTA BIPE DE RECEBIMENTO (EXPEDIDO E N√ÉO CHEGOU)", "EM TR√ÇNSITO PARA A BASE",
        "FALTA BIPE DE RECEBIMENTO (EXPEDIDO E N√ÉO CHEGOU)", "EM TR√ÇNSITO PARA A BASE",
    ]
    df[COL_TRANSITO] = np.select(conditions, choices, default='')
    logging.info("Regras de tr√¢nsito aplicadas com sucesso.")
    return df

# ==============================================================================
# --- FUN√á√ÉO DE STATUS CORRIGIDA E AMPLIADA ---
# ==============================================================================
def aplicar_regras_status(df: pd.DataFrame) -> pd.DataFrame:
    """
    Aplica regras de neg√≥cio para definir o 'Status'.
    A l√≥gica original foi mantida e as novas regras de "Exceed X days" foram adicionadas
    como um caso geral, avaliado ap√≥s as regras mais espec√≠ficas.
    """
    logging.info("Aplicando regras de status (l√≥gica original + novas regras de dias)...")

    # Manter a l√≥gica original para condi√ß√µes espec√≠ficas
    is_problematico = df[COL_ULTIMA_OPERACAO] == "ÈóÆÈ¢ò‰ª∂Êâ´Êèè/Bipe de pacote problem√°tico"
    is_envio_errado_cd = df[COL_BASE_RECENTE].isin(BASES_CD) & df[COL_REGIONAL].isin(BASES_CD)

    # Regras de neg√≥cio ESPEC√çFICAS (mantidas do original)
    regras_especificas: List[Dict[str, Any]] = [
        # Extravio
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Extravio.interno.ÂÜÖÈÉ®ÈÅóÂ§±"), "status": "PEDIDO EXTRAVIADO"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Encomenda.expedido.mas.n√£o.chegou.ÊúâÂèëÊú™Âà∞‰ª∂") & (df[COL_DIAS_PARADO] >= 3), "status": "ALERTA DE EXTRAVIO: ABRIR CHAMADO INTERNO (H√Å MAIS DE 3 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Encomenda.expedido.mas.n√£o.chegou.ÊúâÂèëÊú™Âà∞‰ª∂"), "status": "ATEN√á√ÉO: RISCO DE EXTRAVIO (AGUARDANDO CHEGADA)"},
        # Retidos
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "retidos.Áïô‰ªì") & (df[COL_DIAS_PARADO] >= 3), "status": "ATEN√á√ÉO: PACOTE RETIDO NO PISO (H√Å MAIS DE 3 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "retidos.Áïô‰ªì"), "status": "ATEN√á√ÉO: PACOTE RETIDO NO PISO"},
        # Endere√ßo
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin(["Endere√ßo.incorretoÂú∞ÂùÄ‰ø°ÊÅØÈîôËØØ", "Impossibilidade.de.chegar.no.endere√ßo.informadoÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•", "Endere√ßo.incompletoÂú∞ÂùÄ‰ø°ÊÅØ‰∏çËØ¶", "Impossibilidade.de.chegar.no.endere√ßo.informado.de.coleta.ÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•C"])) & (df[COL_DIAS_PARADO] >= 8), "status": "SOLICITAR DEVOLU√á√ÉO (ENDERE√áO/ACESSO INCORRETO, H√Å MAIS DE 8 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin(["Endere√ßo.incorretoÂú∞ÂùÄ‰ø°ÊÅØÈîôËØØ", "Impossibilidade.de.chegar.no.endere√ßo.informadoÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•", "Endere√ßo.incompletoÂú∞ÂùÄ‰ø°ÊÅØ‰∏çËØ¶", "Impossibilidade.de.chegar.no.endere√ßo.informado.de.coleta.ÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•C"])), "status": "ATEN√á√ÉO: AGUARDANDO DEVOLU√á√ÉO (ENDERE√áO/ACESSO INCORRETO)"},
        # Tentativas de entrega / aus√™ncia
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Aus√™ncia.de.destinat√°rio.nas.v√°rias.tentativas.de.entregaÂ§öÊ¨°Ê¥æÈÄÅÂÆ¢Êà∑‰∏çÂú®"), "status": "VERIFICAR 3 TENTATIVAS DE ENTREGA. SE OK, SOLICITAR DEVOLU√á√ÉO. SEN√ÉO, REALIZAR NOVA TENTATIVA."},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Aus√™ncia.do.destinat√°rioÂÆ¢Êà∑‰∏çÂú®") & (df[COL_DIAS_PARADO] >= 2), "status": "ATEN√á√ÉO: DEVOLVER √Ä BASE (AUS√äNCIA, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Aus√™ncia.do.destinat√°rioÂÆ¢Êà∑‰∏çÂú®"), "status": "ATEN√á√ÉO: DEVOLU√á√ÉO √Ä BASE PENDENTE (AUS√äNCIA)"},
        # Recusa / mudan√ßa de endere√ßo
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin(["Recusa.de.recebimento.pelo.cliente.(destinat√°rio)Êó†ÁêÜÁî±ÊãíÊî∂", "O.destinat√°rio.mudou.o.endere√ßo.Êî∂‰ª∂‰∫∫Êê¨ÂÆ∂"])) & (df[COL_DIAS_PARADO] >= 2), "status": "ATEN√á√ÉO: DEVOLVER √Ä BASE (RECUSA/MUDAN√áA DE ENDERE√áO, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin(["Recusa.de.recebimento.pelo.cliente.(destinat√°rio)Êó†ÁêÜÁî±ÊãíÊî∂", "O.destinat√°rio.mudou.o.endere√ßo.Êî∂‰ª∂‰∫∫Êê¨ÂÆ∂"])), "status": "ATEN√á√ÉO: DEVOLU√á√ÉO √Ä BASE PENDENTE (RECUSA/MUDAN√áA DE ENDERE√áO)"},
        # Outros problem√°ticos
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin(["Pacote.fora.do.padr√£o.‰∏âËæπÂ∞∫ÂØ∏Ë∂ÖÈôê", "Embalagem.n√£o.conforme.ÂåÖË£Ö‰∏çËßÑËåÉ"])), "status": "SOLICITAR DEVOLU√á√ÉO IMEDIATA (FORA DO PADR√ÉO / EMBALAGEM N√ÉO CONFORME)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Mercadorias.que.chegam.incompletosË¥ßÊú™Âà∞ÈΩê") & (df[COL_DIAS_PARADO] >= 2), "status": "ENVIAR PARA O FLUXO INVERSO (INCOMPLETO, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Pacotes.retidos.por.anomalias.ÂºÇÂ∏∏Êã¶Êà™‰ª∂") & (df[COL_DIAS_PARADO] >= 3), "status": "ENVIAR PARA A QUALIDADE (ANOMALIA, H√Å MAIS DE 3 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Pacotes.retidos.por.anomalias.ÂºÇÂ∏∏Êã¶Êà™‰ª∂"), "status": "ATEN√á√ÉO: ANOMALIA EM AN√ÅLISE"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Devolu√ß√£o.ÈÄÄÂõû‰ª∂"), "status": "ENVIAR PARA SC/DC (DEVOLU√á√ÉO APROVADA)"},
        # Opera√ß√µes Normais
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega") & (df[COL_REGIONAL].isin(FRANQUIAS)) & (df[COL_DIAS_PARADO] >= 2), "status": "ATRASO NA ENTREGA (FRANQUIA)"},
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega") & (df[COL_REGIONAL].isin(FRANQUIAS)), "status": "EM ROTA DE ENTREGA (FRANQUIA)"},
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega") & (~df[COL_REGIONAL].isin(FRANQUIAS)) & (df[COL_DIAS_PARADO] >= 2), "status": "ATEN√á√ÉO: ATRASO NA ENTREGA (BASE PR√ìPRIA)"},
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega"), "status": "EM ROTA DE ENTREGA (BASE PR√ìPRIA)"},
        # Envio Errado (CDs)
        {"condicao": is_envio_errado_cd & (df[COL_NOME_PROBLEMATICO] == "Mercadorias.do.cliente.n√£o.est√£o.completas.ÂÆ¢Êà∑Ë¥ßÁâ©Êú™Â§áÈΩê"), "status": "ENVIAR PARA O FLUXO INVERSO (INCOMPLETO, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_envio_errado_cd & (df[COL_NOME_PROBLEMATICO] == "Aus√™ncia.do.destinat√°rioÂÆ¢Êà∑‰∏çÂú®"), "status": "VERIFICAR 3 TENTATIVAS DE ENTREGA. SE OK, SOLICITAR DEVOLU√á√ÉO. SEN√ÉO, REALIZAR NOVA TENTATIVA."},
        {"condicao": is_envio_errado_cd, "status": "ENVIO ERRADO - ENTRE CDs"},
    ]

    # Regras de neg√≥cio GERAIS baseadas nos dias (as novas regras)
    # Estas regras s√≥ ser√£o aplicadas se nenhuma das regras espec√≠ficas acima for correspondida.
    regras_gerais_dias: List[Dict[str, Any]] = [
        {"condicao": df[COL_DIAS_PARADO] >= 30, "status": "Exceed 30 days with no track"},
        {"condicao": df[COL_DIAS_PARADO] >= 14, "status": "Exceed 14 days with no track"},
        {"condicao": df[COL_DIAS_PARADO] >= 10, "status": "Exceed 10 days with no track"},
        {"condicao": df[COL_DIAS_PARADO] >= 7,  "status": "Exceed 7 days with no track"},
        {"condicao": df[COL_DIAS_PARADO] >= 6,  "status": "Exceed 6 days with no track"},
        {"condicao": df[COL_DIAS_PARADO] >= 5,  "status": "Exceed 5 days with no track"},
    ]

    # Combinar as regras espec√≠ficas e gerais em uma √∫nica lista
    # A ordem √© crucial: as espec√≠ficas primeiro, as gerais depois.
    todas_as_regras = regras_especificas + regras_gerais_dias

    conditions = [regra["condicao"] for regra in todas_as_regras]
    choices = [regra["status"] for regra in todas_as_regras]

    # O valor padr√£o ser√° a opera√ß√£o em mai√∫sculas, caso nenhuma regra se aplique.
    df[COL_STATUS] = np.select(conditions, choices, default=df[COL_ULTIMA_OPERACAO].str.upper())

    logging.info("Regras de status aplicadas com sucesso.")
    return df


def calcular_multa(df: pd.DataFrame) -> pd.DataFrame:
    """Calcula o valor da multa para pacotes com 6 ou mais dias de atraso."""
    if df.empty:
        logging.info("Nenhum pacote com 6+ dias para c√°lculo de multa.")
        return df
    logging.info("Calculando multa para pacotes com 6 ou mais dias parados...")
    df_copy = df.copy()
    condicoes_multa = [
        df_copy[COL_DIAS_PARADO] >= 30,
        df_copy[COL_DIAS_PARADO].between(14, 29),
        df_copy[COL_DIAS_PARADO].between(10, 13),
        df_copy[COL_DIAS_PARADO].between(7, 9),
        df_copy[COL_DIAS_PARADO] == 6,
        df_copy[COL_DIAS_PARADO] == 5,
        df_copy[COL_DIAS_PARADO] == 4,
        df_copy[COL_DIAS_PARADO] == 3,
        df_copy[COL_DIAS_PARADO] == 2,
    ]
    valores_multa = [30, 14, 10, 7, 6, 5, 4, 3, 2]
    df_copy[COL_MULTA] = np.select(condicoes_multa, valores_multa, default=0)
    logging.info("Multa calculada por item.")
    return df_copy


def processar_dados(df_main: pd.DataFrame, df_problematicos: pd.DataFrame, df_devolucao: pd.DataFrame) -> pd.DataFrame:
    """Orquestra todo o processo de enriquecimento e c√°lculo sobre o DataFrame principal."""
    colunas_necessarias = [COL_REMESSA, COL_ULTIMA_OPERACAO, COL_REGIONAL, COL_NOME_PROBLEMATICO, COL_HORA_OPERACAO, COL_BASE_RECENTE]
    if not all(col in df_main.columns for col in colunas_necessarias):
        colunas_faltantes = set(colunas_necessarias) - set(df_main.columns)
        logging.critical(f"O arquivo principal n√£o cont√©m as colunas obrigat√≥rias: {colunas_faltantes}.")
        return pd.DataFrame()

    df = df_main.copy()
    df[COL_REMESSA] = df[COL_REMESSA].astype(str)

    # 1. Processar e mesclar dados de Pacotes Problem√°ticos
    if not df_problematicos.empty:
        df_problematicos['N√∫mero de pedido JMS'] = df_problematicos['N√∫mero de pedido JMS'].astype(str)
        df_problematicos['Tempo de digitaliza√ß√£o'] = pd.to_datetime(df_problematicos['Tempo de digitaliza√ß√£o'], errors='coerce')
        df_problematicos = df_problematicos.sort_values('Tempo de digitaliza√ß√£o').dropna(subset=['N√∫mero de pedido JMS'])
        summary = df_problematicos.groupby('N√∫mero de pedido JMS').agg(
            Qtd_Problematicas=('N√∫mero de pedido JMS', 'size'),
            Ultima_Problematica_Detalhada=('Tipo de n√≠vel II de pacote problem√°tico', 'last')
        ).reset_index()
        df = df.merge(summary, left_on=COL_REMESSA, right_on='N√∫mero de pedido JMS', how='left')
        df.drop(columns=['N√∫mero de pedido JMS'], inplace=True)
        logging.info("Dados de pacotes problem√°ticos integrados.")

    df['√öltima Problem√°tica Detalhada'] = df.get('Ultima_Problematica_Detalhada', pd.Series(index=df.index)).fillna('-')
    df['Qtd Problem√°ticas'] = df.get('Qtd_Problematicas', pd.Series(index=df.index)).fillna(0).astype(int)

    # 2. Processar e mesclar dados de Devolu√ß√µes
    df[COL_DEVOLUCAO] = 'DEVOLU√á√ÉO N√ÉO SOLICITADA'
    if not df_devolucao.empty:
        df_devolucao['N√∫mero de pedido JMS'] = df_devolucao['N√∫mero de pedido JMS'].astype(str)
        mapa_traducao = {'ÂæÖÂÆ°Ê†∏': 'EM PROCESSO DE APROVA√á√ÉO', 'È©≥Âõû': 'PEDIDO DE DEVOLU√á√ÉO RECUSADO', 'Â∑≤ÂÆ°Ê†∏': 'DEVOLU√á√ÉO APROVADA'}
        df_devolucao['Status_Traduzido'] = df_devolucao['Estado de solicita√ß√£o'].map(mapa_traducao)
        df_devolucao_info = df_devolucao.dropna(subset=['Status_Traduzido'])[['N√∫mero de pedido JMS', 'Status_Traduzido']].drop_duplicates(subset='N√∫mero de pedido JMS', keep='last')
        df = df.merge(df_devolucao_info, left_on=COL_REMESSA, right_on='N√∫mero de pedido JMS', how='left')
        df[COL_DEVOLUCAO] = df['Status_Traduzido'].fillna(df[COL_DEVOLUCAO])
        df.drop(columns=['N√∫mero de pedido JMS', 'Status_Traduzido'], inplace=True, errors='ignore')
        logging.info("Dados de devolu√ß√£o integrados.")

    # 3. C√°lculo de Dias Parado (antes das regras)
    logging.info("Calculando dias parados a partir da coluna 'Aging'...")
    if df[COL_HORA_OPERACAO].dtype == 'object':
        try:
            df[COL_DIAS_PARADO] = pd.to_numeric(df[COL_HORA_OPERACAO], errors='coerce').fillna(0).astype(int)
            logging.info("Coluna 'Aging' convertida diretamente para n√∫mero de dias.")
        except Exception as e:
            logging.error(f"Erro ao converter coluna 'Aging': {e}")
            df[COL_DIAS_PARADO] = 0
    elif np.issubdtype(df[COL_HORA_OPERACAO].dtype, np.number):
        df[COL_DIAS_PARADO] = df[COL_HORA_OPERACAO].fillna(0).astype(int)
        logging.info("Coluna 'Aging' j√° √© num√©rica, usando diretamente.")
    else:
        df[COL_HORA_OPERACAO] = pd.to_datetime(df[COL_HORA_OPERACAO], errors='coerce')
        df[COL_DIAS_PARADO] = (datetime.now() - df[COL_HORA_OPERACAO]).dt.days.fillna(0).astype(int)
        logging.info("Coluna 'Aging' √© datetime, calculando dias.")

    # 4. Aplica√ß√£o de Regras de Neg√≥cio
    df = aplicar_regras_status(df) # Usa a fun√ß√£o de status corrigida
    df = aplicar_regras_transito(df)

    # 5. Ajustes Finais (RESTAURADO)
    # A linha abaixo foi restaurada, pois a sobrescrita por devolu√ß√£o √© uma regra de neg√≥cio importante.
    df.loc[df[COL_DEVOLUCAO] != 'DEVOLU√á√ÉO N√ÉO SOLICITADA', COL_STATUS] = df[COL_DEVOLUCAO]
    cond_aprovado_em_rota = (df[COL_STATUS] == 'DEVOLU√á√ÉO APROVADA') & (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega")
    df.loc[cond_aprovado_em_rota, COL_STATUS] = 'DEVOLU√á√ÉO APROVADA, MAS O PACOTE EST√Å EM ROTA'
    condicao_aprovado = df[COL_STATUS].isin(['DEVOLU√á√ÉO APROVADA', 'DEVOLU√á√ÉO APROVADA, MAS O PACOTE EST√Å EM ROTA'])
    df.loc[condicao_aprovado, COL_TRANSITO] = ''

    df.rename(columns={COL_REGIONAL: COLUNA_CHAVE_PRINCIPAL}, inplace=True)

    ordem_colunas = [COL_REMESSA, COLUNA_CHAVE_PRINCIPAL, COL_DIAS_PARADO, COL_ULTIMA_OPERACAO, COL_HORA_OPERACAO, COL_STATUS, COL_TRANSITO, COL_DEVOLUCAO, 'Qtd Problem√°ticas', '√öltima Problem√°tica Detalhada']
    colunas_existentes = [col for col in df.columns if col not in ordem_colunas]
    df = df[ordem_colunas + colunas_existentes]
    return df

def adicionar_info_coordenador(df_principal: pd.DataFrame) -> pd.DataFrame:
    """Adiciona informa√ß√µes de Coordenador e Filial ao DataFrame com base em um arquivo de mapeamento."""
    if df_principal.empty:
        logging.warning("DataFrame de entrada est√° vazio. Pulando adi√ß√£o de coordenadores.")
        return df_principal
    try:
        logging.info(f"Lendo arquivo de mapeamento: {os.path.basename(ARQUIVO_MAPEAMENTO_COORDENADORES)}")
        df_mapeamento = pd.read_excel(ARQUIVO_MAPEAMENTO_COORDENADORES)
    except FileNotFoundError:
        logging.error(f"ERRO CR√çTICO: Arquivo de mapeamento '{ARQUIVO_MAPEAMENTO_COORDENADORES}' n√£o encontrado. O processo ser√° interrompido.")
        raise
    except Exception as e:
        logging.error(f"Ocorreu um erro ao ler o arquivo de mapeamento: {e}. O processo ser√° interrompido.")
        raise
    mapa_coordenador = pd.Series(df_mapeamento[COLUNA_INFO_COORDENADOR].values, index=df_mapeamento[COLUNA_CHAVE_MAPEAMENTO]).to_dict()
    mapa_filial = pd.Series(df_mapeamento[COLUNA_INFO_FILIAL].values, index=df_mapeamento[COLUNA_CHAVE_MAPEAMENTO]).to_dict()
    logging.info("Adicionando informa√ß√µes de Coordenador e Filial...")
    key_series = df_principal[COLUNA_CHAVE_PRINCIPAL]
    if isinstance(key_series, pd.DataFrame):
        logging.warning(f"Aten√ß√£o: Colunas duplicadas encontradas para '{COLUNA_CHAVE_PRINCIPAL}'. Usando a primeira ocorr√™ncia.")
        key_series = key_series.iloc[:, 0]
    df_principal[NOVA_COLUNA_COORDENADOR] = key_series.map(mapa_coordenador).fillna('N√ÉO ENCONTRADO')
    df_principal[NOVA_COLUNA_FILIAL] = key_series.map(mapa_filial).fillna('N√ÉO ENCONTRADA')
    logging.info("Informa√ß√µes de coordenador e filial adicionadas.")
    return df_principal


# ==============================================================================
# --- FUN√á√ÉO PARA SALVAR RELAT√ìRIOS ---
# ==============================================================================
def salvar_relatorios(df_final: pd.DataFrame, pasta_saida: str):
    if df_final.empty:
        logging.warning("‚ö†Ô∏è Nenhum dado para salvar relat√≥rios.")
        return
    data_hoje = datetime.now().strftime("%Y-%m-%d")
    try:
        resumo = {"0-4 dias": (df_final[COL_DIAS_PARADO] <= 4).sum(), "5+ dias": (df_final[COL_DIAS_PARADO] >= 5).sum(), "Total": len(df_final)}
        logging.info(f"üìä Resumo Dias Parados: {resumo}")
    except Exception as e:
        logging.error(f"Erro ao gerar resumo de debug: {e}")
    df_0_4 = df_final[df_final[COL_DIAS_PARADO] <= 4]
    if not df_0_4.empty:
        arquivo_0_4 = os.path.join(pasta_saida, f"Relat√≥rio Sem Movimenta√ß√£o (0-4 dias)_{data_hoje}.xlsx")
        df_0_4.to_excel(arquivo_0_4, index=False)
        logging.info(f"Relat√≥rio 0-4 dias salvo: {arquivo_0_4}")
    df_5_plus = df_final[df_final[COL_DIAS_PARADO] >= 5]
    if not df_5_plus.empty:
        df_5_plus = calcular_multa(df_5_plus)
        arquivo_5_plus = os.path.join(pasta_saida, f"Relat√≥rio Sem Movimenta√ß√£o (5+ dias)_{data_hoje}.xlsx")
        df_5_plus.to_excel(arquivo_5_plus, index=False)
        logging.info(f"‚úÖ Relat√≥rio 5+ dias salvo: {arquivo_5_plus}")
    else:
        logging.warning("‚ö†Ô∏è Nenhum pedido encontrado com 5+ dias parados.")
    df_incompletos = df_final[df_final[COL_NOME_PROBLEMATICO] == "Mercadorias.que.chegam.incompletosË¥ßÊú™Âà∞ÈΩê"]
    if not df_incompletos.empty:
        arquivo_incompletos = os.path.join(pasta_saida, f"Relat√≥rio Mercadorias incompletas_{data_hoje}.xlsx")
        df_incompletos.to_excel(arquivo_incompletos, index=False)
        logging.info(f"‚úÖ Relat√≥rio Mercadorias incompletas salvo: {arquivo_incompletos}")


# ==============================================================================
# --- FUN√á√ÉO PARA MOVER RELAT√ìRIOS ANTIGOS PARA ARQUIVO MORTO ---
# ==============================================================================
def mover_para_arquivo_morto(pasta_origem: str, pasta_destino: str):
    """Move relat√≥rios antigos para a pasta de Arquivo Morto."""
    if not os.path.exists(pasta_destino): os.makedirs(pasta_destino)
    hoje = datetime.now().strftime("%Y-%m-%d")
    arquivos = [f for f in os.listdir(pasta_origem) if f.endswith(('.xlsx', '.xls'))]
    arquivos_hoje = [f for f in arquivos if hoje in f]
    arquivos_hoje.sort(key=lambda f: os.path.getmtime(os.path.join(pasta_origem, f)))
    if len(arquivos_hoje) > 1:
        for arquivo in arquivos_hoje[:-1]:
            try: shutil.move(os.path.join(pasta_origem, arquivo), os.path.join(pasta_destino, arquivo)); logging.info(f"üì¶ Arquivo duplicado de hoje movido: {arquivo}")
            except Exception as e: logging.error(f"Erro ao mover o arquivo {arquivo}: {e}")
    for arquivo in arquivos:
        if arquivo not in arquivos_hoje:
            try: shutil.move(os.path.join(pasta_origem, arquivo), os.path.join(pasta_destino, arquivo)); logging.info(f"üì¶ Arquivo antigo movido para Arquivo Morto: {arquivo}")
            except Exception as e: logging.error(f"Erro ao mover o arquivo {arquivo}: {e}")

# ==============================================================================
# --- MAIN ---
# ==============================================================================
def main():
    logging.info("--- INICIANDO PROCESSO DE GERA√á√ÉO DE RELAT√ìRIOS ---")
    caminho_arquivo_original = encontrar_arquivo_principal(PATH_INPUT_MAIN, FILENAME_START_MAIN)
    if not caminho_arquivo_original:
        logging.critical("Arquivo principal n√£o encontrado. Processo interrompido.")
        return
    df_main = pd.read_excel(caminho_arquivo_original)
    df_problematicos = carregar_planilhas_de_pasta(PATH_INPUT_PROBLEMATICOS, "Consolidando problem√°ticos")
    df_devolucao = carregar_planilhas_de_pasta(PATH_INPUT_DEVOLUCAO, "Consolidando devolu√ß√µes")
    df_final = processar_dados(df_main, df_problematicos, df_devolucao)
    df_final = adicionar_info_coordenador(df_final)
    mover_para_arquivo_morto(PATH_OUTPUT_REPORTS, PATH_OUTPUT_ARQUIVO_MORTO)
    salvar_relatorios(df_final, PATH_OUTPUT_REPORTS)
    logging.info("--- PROCESSO CONCLU√çDO COM SUCESSO! ---")

if __name__ == "__main__":
    main()