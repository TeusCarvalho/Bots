# -*- coding: utf-8 -*-
"""
ğŸ“¦ RelatÃ³rio de Pacotes Retidos - Regional GP
VersÃ£o 2.5 Multi-Auto Read Pro ğŸ’¼
Autor: bb (ChatGPT â¤ï¸ Matheus)

âœ¨ Recursos:
- LÃª automaticamente TODOS os arquivos .xlsx da pasta
- Busca automÃ¡tica da Base_Atualizada.xlsx
- Reconhecimento flexÃ­vel de nomes de colunas
- ExportaÃ§Ã£o para OneDrive
- Card interativo no Feishu com link do SharePoint
- CriaÃ§Ã£o automÃ¡tica de pastas
"""

import pandas as pd
import requests
import logging
from datetime import datetime
from pathlib import Path

# ==========================================================
# ğŸ¨ CONFIGURAÃ‡ÃƒO DE LOGS
# ==========================================================
logging.basicConfig(format="%(asctime)s - %(levelname)s: %(message)s", level=logging.INFO)
log = logging.getLogger()

# ==========================================================
# ğŸš€ ENVIO DE CARD PARA FEISHU
# ==========================================================
def send_to_feishu_bot(webhook_url: str, message_content: dict) -> None:
    headers = {'Content-Type': 'application/json'}
    try:
        response = requests.post(webhook_url, headers=headers, json=message_content)
        response.raise_for_status()
        log.info("âœ… Card enviado com sucesso ao Feishu.")
    except requests.exceptions.RequestException as e:
        log.error(f"âŒ Erro ao enviar mensagem ao Feishu: {e}")

# ==========================================================
# ğŸ” BUSCA AUTOMÃTICA DE PLANILHA DE COORDENADORES
# ==========================================================
def find_coordinators_file(base_dir: Path, filename: str = "Base_Atualizada.xlsx") -> Path | None:
    log.info(f"ğŸ” Procurando '{filename}' dentro de '{base_dir}' ...")
    for file in base_dir.rglob(filename):
        log.info(f"âœ… Arquivo encontrado: {file}")
        return file
    log.error(f"âŒ Arquivo '{filename}' nÃ£o foi encontrado em '{base_dir}' ou subpastas.")
    return None

# ==========================================================
# ğŸ§  FUNÃ‡ÃƒO DE PROCESSAMENTO DE UM ARQUIVO
# ==========================================================
def process_file(file_path: Path, df_coordenadores: pd.DataFrame, sheet_name: str, output_dir: Path,
                 feishu_webhook_url: str, sharepoint_link: str) -> None:
    try:
        log.info(f"ğŸ“„ Processando arquivo: {file_path.name}")
        excel = pd.ExcelFile(file_path)
        if sheet_name not in excel.sheet_names:
            sheet_name = excel.sheet_names[0]
            log.warning(f"Aba '{sheet_name}' detectada automaticamente.")

        df_main = pd.read_excel(excel, sheet_name=sheet_name)
        df_main.columns = df_main.columns.str.strip()
        df_coordenadores.columns = df_coordenadores.columns.str.strip()

        # ==========================================================
        # ğŸ”  DETECÃ‡ÃƒO AUTOMÃTICA DE NOMES DE COLUNAS
        # ==========================================================
        possible_columns = {
            'regional nova åŒºåŸŸ': ['regional nova åŒºåŸŸ', 'Regional åŒºåŸŸ', 'regional åŒºåŸŸ', 'Regional Nova åŒºåŸŸ'],
            'Base de Entrega æ´¾ä»¶ç½‘ç‚¹': ['Base de Entrega æ´¾ä»¶ç½‘ç‚¹', 'Base Entrega æ´¾ä»¶ç½‘ç‚¹', 'ç½‘ç‚¹åç§°'],
            'NÃºmero do Pedido JMS è¿å•å·': ['NÃºmero do Pedido JMS è¿å•å·', 'è¿å•å·', 'NÃºmero Pedido JMS'],
            'Cluster Retidos åˆ†ç±»': ['Cluster Retidos åˆ†ç±»', 'åˆ†ç±»', 'Cluster Retido']
        }

        for canonical, variations in possible_columns.items():
            for var in variations:
                if var in df_main.columns:
                    df_main.rename(columns={var: canonical}, inplace=True)
                    log.info(f"ğŸ“‹ Coluna '{var}' reconhecida como '{canonical}'")
                    break
            else:
                log.error(f"âŒ Nenhuma variaÃ§Ã£o encontrada para a coluna '{canonical}' no arquivo {file_path.name}")
                return

        # ==========================================================
        # ğŸ”¢ PROCESSAMENTO
        # ==========================================================
        df_merged = df_main.merge(
            df_coordenadores,
            left_on='Base de Entrega æ´¾ä»¶ç½‘ç‚¹',
            right_on='Nome da base',
            how='left'
        )

        df_gp = df_merged[df_merged['regional nova åŒºåŸŸ'] == 'GP'].copy()
        log.info(f"ğŸ¯ Regional 'GP' â€” {len(df_gp)} registros encontrados.")

        if df_gp.empty:
            log.warning(f"âš ï¸ Nenhum registro 'GP' em {file_path.name}")
            return

        contagem_total = df_gp.groupby('Coordenadores')['NÃºmero do Pedido JMS è¿å•å·'].count()
        contagem_detalhada = df_gp.groupby(['Coordenadores', 'Cluster Retidos åˆ†ç±»'])['NÃºmero do Pedido JMS è¿å•å·'].count()

        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        output_path = output_dir / f"Resultados_{file_path.stem}_{timestamp}.xlsx"
        with pd.ExcelWriter(output_path) as writer:
            contagem_total.to_excel(writer, sheet_name='Contagem Total por Coordenador')
            contagem_detalhada.to_excel(writer, sheet_name='Contagem por Coordenador e Dia')
            df_gp.to_excel(writer, sheet_name='Dados Filtrados', index=False)
        log.info(f"ğŸ’¾ Resultado salvo: {output_path}")

        # ==========================================================
        # ğŸ’¬ CARD FEISHU
        # ==========================================================
        total_pacotes = len(df_gp)
        coordenador_cards = []
        for coordenador, qtd in contagem_total.items():
            qtd_por_dia = ""
            try:
                for dia, qtd_dia in contagem_detalhada.loc[coordenador].items():
                    qtd_por_dia += f"- {dia}: {qtd_dia} pedidos\n"
            except KeyError:
                qtd_por_dia = "Nenhum detalhe por dia.\n"
            coordenador_cards.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"**ğŸ“ {coordenador}** â€” {qtd} pacotes\n{qtd_por_dia}"
                }
            })

        feishu_message_content = {
            "msg_type": "interactive",
            "card": {
                "config": {"wide_screen_mode": True},
                "header": {
                    "title": {"tag": "plain_text", "content": f"ğŸ“¦ RelatÃ³rio - {file_path.stem}"},
                    "template": "blue"
                },
                "elements": [
                    {"tag": "div", "text": {"tag": "lark_md",
                                            "content": f"**Total de Pacotes Retidos:** {total_pacotes} ğŸ“¦"}},
                    {"tag": "hr"},
                    *coordenador_cards,
                    {"tag": "div", "text": {"tag": "lark_md",
                                            "content": f"ğŸ“ [Acessar no SharePoint]({sharepoint_link})"}}
                ]
            }
        }

        send_to_feishu_bot(feishu_webhook_url, feishu_message_content)

    except Exception as e:
        log.error(f"âŒ Erro ao processar {file_path.name}: {e}")

# ==========================================================
# ğŸ§© EXECUÃ‡ÃƒO PRINCIPAL
# ==========================================================
if __name__ == '__main__':
    # Caminho onde estÃ£o os arquivos a processar
    caminho_da_pasta = Path(
        r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda\Teste\Retidos'
    )

    if not caminho_da_pasta.exists():
        caminho_da_pasta.mkdir(parents=True, exist_ok=True)
        log.warning(f"ğŸ“ Pasta '{caminho_da_pasta}' nÃ£o existia e foi criada automaticamente.")

    # Pasta base onde estÃ¡ o arquivo de coordenadores
    base_test_dir = Path(
        r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda (1)\Ãrea de Trabalho\Testes'
    )

    feishu_url = 'https://open.feishu.cn/open-apis/bot/v2/hook/b8328e19-9b9f-40d5-bce0-6af7f4612f1b'
    sharepoint_link = (
        "https://jtexpressdf-my.sharepoint.com/:f:/g/personal/"
        "matheus_carvalho_jtexpressdf_onmicrosoft_com/"
        "Ep7sv6B_nKBMg_S_Tdibe0MB4x--uJseBYT52EiRTEqzyA?e=hcTca7"
    )
    nome_da_aba = 'æ»ç•™æ˜ç»†è¡¨'

    # Localiza planilha de coordenadores
    coordinators_file_path = find_coordinators_file(base_test_dir)
    if not coordinators_file_path:
        exit()

    df_coordenadores = pd.read_excel(coordinators_file_path)
    arquivos = list(caminho_da_pasta.glob('*.xlsx'))

    if not arquivos:
        log.warning("âš ï¸ Nenhum arquivo .xlsx encontrado na pasta de Retidos.")
        exit()

    log.info(f"ğŸ“Š {len(arquivos)} arquivo(s) encontrados. Iniciando processamento...\n")

    for arquivo in arquivos:
        process_file(arquivo, df_coordenadores, nome_da_aba, caminho_da_pasta,
                     feishu_url, sharepoint_link)

    log.info("âœ… Processamento finalizado com sucesso!")
