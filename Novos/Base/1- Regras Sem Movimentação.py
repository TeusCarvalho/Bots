# -*- coding: utf-8 -*-
import pandas as pd
import os
import numpy as np
from tqdm import tqdm
from datetime import datetime
import shutil
import logging
from typing import List, Dict, Optional, Any

# ==============================================================================
# --- CONFIGURA√á√ÉO GERAL ---
# ==============================================================================

# --- 1. Caminhos Principais ---
BASE_PATH = r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda (1)\√Årea de Trabalho\Testes\Sem Movimenta√ß√£o'
OUTPUT_BASE_PATH = r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda\Jt - Relat√≥rios'
COORDENADOR_BASE_PATH = r'C:\Users\J&T-099\OneDrive - Speed Rabbit Express Ltda (1)\√Årea de Trabalho\Testes\Coordenador'

# --- 2. Pastas e Arquivos de Entrada ---
PATH_INPUT_MAIN = os.path.join(BASE_PATH, 'Sem_Movimenta√ß√£o')
PATH_INPUT_PROBLEMATICOS = os.path.join(BASE_PATH, 'Pacotes Problematicos')
PATH_INPUT_DEVOLUCAO = os.path.join(BASE_PATH, 'Devolu√ß√£o')
ARQUIVO_MAPEAMENTO_COORDENADORES = os.path.join(COORDENADOR_BASE_PATH, 'Base_Atualizada.xlsx')

# --- 3. Pastas de Sa√≠da ---
PATH_OUTPUT_REPORTS = OUTPUT_BASE_PATH
PATH_OUTPUT_ARQUIVO_MORTO = os.path.join(OUTPUT_BASE_PATH, "Arquivo Morto")

# --- 4. Nomes de Arquivos e Colunas ---
FILENAME_START_MAIN = 'Monitoramento de movimenta√ß√£o em tempo real'

# Colunas principais
COL_REMESSA = 'Remessa'
COL_DIAS_PARADO = 'Dias Parado'
COL_ULTIMA_OPERACAO = 'Tipo da √∫ltima opera√ß√£o'
COL_REGIONAL = 'Regional respons√°vel'
COL_NOME_PROBLEMATICO = 'Nome de pacote problem√°tico'
COL_HORA_OPERACAO = 'Hor√°rio da √∫ltima opera√ß√£o'
COL_DEVOLUCAO = 'Devolu√ß√£o'
COL_STATUS = 'Status'
COL_MULTA = 'Multa (R$)'
COL_BASE_RECENTE = 'Nome da base mais recente'
COL_TRANSITO = 'Tr√¢nsito'

# Colunas para mapeamento de coordenadores
COLUNA_CHAVE_PRINCIPAL = 'Unidade respons√°vel'
COLUNA_CHAVE_MAPEAMENTO = 'Nome da base'
COLUNA_INFO_COORDENADOR = 'Coordenadores'
COLUNA_INFO_FILIAL = 'Filial'
NOVA_COLUNA_COORDENADOR = 'Coordenadores'
NOVA_COLUNA_FILIAL = 'Filial'

# --- 5. Regra: ‚ÄúMercadorias incompletas‚Äù (cria relat√≥rio separado e REMOVE do principal/Feishu) ---
INCOMPLETOS_KEY_EXATA = "Mercadorias.que.chegam.incompletosË¥ßÊú™Âà∞ÈΩê"
INCOMPLETOS_KEY_PARTE_1 = "Mercadorias.que.chegam.incompletos"
INCOMPLETOS_KEY_PARTE_2 = "Ë¥ßÊú™Âà∞ÈΩê"

# --- 6. Listas para Regras de Neg√≥cio ---
FRANQUIAS = ["F AGL-GO", "F ALV-AM", "F APG - GO", "F ARQ - RO", "F BSB-DF", "F CDN-AM", "F CGR - MS", "F FMA-GO",
             "F GYN - GO", "F ITI-PA", "F RVD - GO", "F TRD-GO", "F CGR 02-MS", "F GYN 02-GO", "F OCD-GO", "F PVH-RO",
             "F TGT-DF", "F DOM -PA", "F JCD-PA", "F MCP-AP", "F ORL-PA", "F PCA-PA", "F RDC -PA", "F SFX-PA",
             "F TLA-PA"]

UNIDADES_SC_DC = ["DC AGB-MT", "DC CGR-MS", "DC GYN-GO", "DC JUI-MT", "DC PVH-RO", "DC RBR-AC", "DF BSB", "GYN -GO",
                  "MT CGB"]

BASES_FLUXO_INVERSO = ["VLP -GO", "VHL-RO", "VGR-MT", "VGR 02-MT", "URC -GO", "TRD -GO", "TLL -MS", "TGT -DF",
                       "TGA -MT", "TAR -AC", "SRS -MT", "SNP -MT", "SMD -AC", "SMB -GO", "SMA -GO", "SJA -GO",
                       "SGO -MS", "SEN-GO", "SBN -DF", "SAMS -DF", "SAD -GO", "RVD -GO", "ROO -MT", "RFI -DF",
                       "RDM -RO", "RBR -AC", "RBR 02-AC", "QUI -GO", "QRN -MT", "PVL -MT", "PVH -RO", "PVH 02-RO",
                       "PTD -MT", "PRG -GO", "PRB -MS", "POS -GO", "PON -GO", "PNT-MS", "PLN -GO", "PLDF -DF",
                       "PA GYN-GO", "OCD-GO", "NVT -MT", "NVR -MS", "NDI -MS", "MT CGB", "MDT -MT", "LUZ -GO",
                       "LRV -MT", "JUI -MT", "JTI -GO", "JRD -MS", "JPN -RO", "ITR -GO", "IPR -GO", "GYN -GO",
                       "GYN 07-GO", "GYN 06-GO", "GYN 05-GO", "GYN 04-GO", "GYN 03-GO", "GYN 02-GO", "GUA -DF", "GP",
                       "GNT -MT", "GNA -GO", "GAM -DF", "FMA -GO", "FAI -GO", "F TRD-GO", "F RVD - GO", "F OCD - GO",
                       "F GYN - GO", "F FMA-GO", "F CGR - MS", "F BSB-DF", "F BSB - DF", "F APG - GO", "F AGL-GO",
                       "EMA -DF", "DOU -MS", "CZS -AC", "CXM -MS", "CTN -GO", "CTL -GO", "CRB -MS", "CNF -MT", "CMP-MT",
                       "CHS -MS", "CGR -MS", "CGR 05-MS", "CGR 04-MS", "CGR 03-MS", "CGR 02-MS", "CGB 05-MT",
                       "CGB 04-MT", "CGB 03-MT", "CGB 02-MT", "CEIS -DF", "CEIN -DF", "CCR -MT", "CAPI -GO", "CAN -GO",
                       "BSB -DF", "BGA -MT", "ATF -MT", "ARQ -RO", "ARI -MT", "AQD -MS", "APG -GO", "ANP -GO",
                       "AMB -MS", "AGL -GO", "AGB -MT"]

DESTINOS_FLUXO_INVERSO = ["MAO -AM", "DC AGB-MT", "DC CGR-MS", "DC GYN-GO", "DC JUI-MT", "DC MAO-AM", "DC MRB-PA",
                          "DC PMW-TO", "DC PVH-RO", "DC RBR-AC", "DC STM-PA", "DF BSB"]

BASES_CD = BASES_FLUXO_INVERSO

# --- Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
def encontrar_arquivo_principal(pasta: str, inicio_nome: str) -> Optional[str]:
    try:
        for nome_arquivo in os.listdir(pasta):
            if nome_arquivo.startswith(inicio_nome) and nome_arquivo.endswith(('.xlsx', '.xls')):
                logging.info(f"Arquivo principal encontrado: {nome_arquivo}")
                return os.path.join(pasta, nome_arquivo)
    except FileNotFoundError:
        logging.error(f"A pasta de leitura '{pasta}' n√£o foi encontrada.")
        return None

    logging.warning(f"Nenhum arquivo come√ßando com '{inicio_nome}' foi encontrado em '{pasta}'.")
    return None


def carregar_planilhas_de_pasta(caminho_pasta: str, descricao_tqdm: str) -> pd.DataFrame:
    lista_dfs = []
    nome_pasta = os.path.basename(caminho_pasta)
    logging.info(f"Lendo planilhas da pasta: {nome_pasta}")

    try:
        arquivos = [f for f in os.listdir(caminho_pasta) if f.endswith(('.xlsx', '.xls'))]
        if not arquivos:
            logging.warning(f"Nenhum arquivo Excel encontrado na pasta '{nome_pasta}'.")
            return pd.DataFrame()

        for arquivo in tqdm(arquivos, desc=descricao_tqdm):
            caminho_completo = os.path.join(caminho_pasta, arquivo)
            try:
                abas = pd.read_excel(caminho_completo, sheet_name=None)
                lista_dfs.extend(abas.values())
            except Exception as e:
                logging.error(f"Falha ao ler o arquivo '{arquivo}' da pasta '{nome_pasta}': {e}")
                continue

        if not lista_dfs:
            return pd.DataFrame()

        df_consolidado = pd.concat(lista_dfs, ignore_index=True)
        logging.info(f"Total de {len(df_consolidado)} registros consolidados de '{nome_pasta}'.")
        return df_consolidado

    except FileNotFoundError:
        logging.error(f"A pasta '{caminho_pasta}' n√£o foi encontrada. Processo interrompido.")
        raise
    except Exception as e:
        logging.error(f"Ocorreu um erro inesperado ao ler os arquivos da pasta '{nome_pasta}': {e}")
        raise
def aplicar_regras_transito(df: pd.DataFrame) -> pd.DataFrame:
    logging.info("Aplicando regras de tr√¢nsito...")

    if COL_BASE_RECENTE not in df.columns:
        logging.warning(f"Coluna '{COL_BASE_RECENTE}' n√£o encontrada. As regras de tr√¢nsito n√£o ser√£o aplicadas.")
        df[COL_TRANSITO] = "COLUNA DE BASE RECENTE N√ÉO ENCONTRADA"
        return df

    cond_em_transito = df[COL_ULTIMA_OPERACAO] == "Âèë‰ª∂Êâ´Êèè/Bipe de expedi√ß√£o"
    is_fluxo_inverso = df[COL_BASE_RECENTE].isin(BASES_FLUXO_INVERSO) & df[COL_REGIONAL].isin(DESTINOS_FLUXO_INVERSO)
    origem_sc_bre = df[COL_BASE_RECENTE] == 'SC BRE'
    destino_pvh = df[COL_REGIONAL].astype(str).str.contains('PVH-RO', na=False, case=False)

    prazo_fluxo_inverso_estourado = df[COL_DIAS_PARADO] >= 3
    prazo_5_dias_estourado = df[COL_DIAS_PARADO] >= 5
    prazo_3_dias_estourado = df[COL_DIAS_PARADO] >= 3

    conditions = [
        cond_em_transito & is_fluxo_inverso & prazo_fluxo_inverso_estourado,
        cond_em_transito & is_fluxo_inverso & ~prazo_fluxo_inverso_estourado,
        cond_em_transito & origem_sc_bre & prazo_5_dias_estourado,
        cond_em_transito & origem_sc_bre & ~prazo_5_dias_estourado,
        cond_em_transito & ~origem_sc_bre & destino_pvh & prazo_5_dias_estourado,
        cond_em_transito & ~origem_sc_bre & destino_pvh & ~prazo_5_dias_estourado,
        cond_em_transito & ~origem_sc_bre & ~destino_pvh & prazo_3_dias_estourado,
        cond_em_transito & ~origem_sc_bre & ~destino_pvh & ~prazo_3_dias_estourado,
    ]
    choices = [
        "VERIFICAR COM TRANSPORTE: VE√çCULO N√ÉO CHEGOU (FLUXO INVERSO)",
        "EM TR√ÇNSITO (FLUXO INVERSO)",
        "FALTA BIPE DE RECEBIMENTO (EXPEDIDO E N√ÉO CHEGOU)",
        "EM TR√ÇNSITO PARA A BASE",
        "FALTA BIPE DE RECEBIMENTO (EXPEDIDO E N√ÉO CHEGOU)",
        "EM TR√ÇNSITO PARA A BASE",
        "FALTA BIPE DE RECEBIMENTO (EXPEDIDO E N√ÉO CHEGOU)",
        "EM TR√ÇNSITO PARA A BASE",
    ]

    df[COL_TRANSITO] = np.select(conditions, choices, default='')
    logging.info("Regras de tr√¢nsito aplicadas com sucesso.")
    return df


def aplicar_regras_status(df: pd.DataFrame) -> pd.DataFrame:
    logging.info("Aplicando regras de status...")

    is_problematico = df[COL_ULTIMA_OPERACAO] == "ÈóÆÈ¢ò‰ª∂Êâ´Êèè/Bipe de pacote problem√°tico"
    is_envio_errado_cd = df[COL_BASE_RECENTE].isin(BASES_CD) & df[COL_REGIONAL].isin(BASES_CD)

    regras: List[Dict[str, Any]] = []

    # 1) PROBLEM√ÅTICOS ‚Äî Extravio
    regras += [
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Extravio.interno.ÂÜÖÈÉ®ÈÅóÂ§±"),
         "status": "PEDIDO EXTRAVIADO"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Encomenda.expedido.mas.n√£o.chegou.ÊúâÂèëÊú™Âà∞‰ª∂") &
                     (df[COL_DIAS_PARADO] >= 3),
         "status": "ALERTA DE EXTRAVIO: ABRIR CHAMADO INTERNO (H√Å MAIS DE 3 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Encomenda.expedido.mas.n√£o.chegou.ÊúâÂèëÊú™Âà∞‰ª∂"),
         "status": "ATEN√á√ÉO: RISCO DE EXTRAVIO (AGUARDANDO CHEGADA)"},
    ]

    # Retidos
    regras += [
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "retidos.Áïô‰ªì") & (df[COL_DIAS_PARADO] >= 3),
         "status": "ATEN√á√ÉO: PACOTE RETIDO NO PISO (H√Å MAIS DE 3 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "retidos.Áïô‰ªì"),
         "status": "ATEN√á√ÉO: PACOTE RETIDO NO PISO"},
    ]

    # Endere√ßo
    regras += [
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin([
            "Endere√ßo.incorretoÂú∞ÂùÄ‰ø°ÊÅØÈîôËØØ",
            "Impossibilidade.de.chegar.no.endere√ßo.informadoÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•",
            "Endere√ßo.incompletoÂú∞ÂùÄ‰ø°ÊÅØ‰∏çËØ¶",
            "Impossibilidade.de.chegar.no.endere√ßo.informado.de.coleta.ÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•C"
        ])) & (df[COL_DIAS_PARADO] >= 8),
         "status": "SOLICITAR DEVOLU√á√ÉO (ENDERE√áO/ACESSO INCORRETO, H√Å MAIS DE 8 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin([
            "Endere√ßo.incorretoÂú∞ÂùÄ‰ø°ÊÅØÈîôËØØ",
            "Impossibilidade.de.chegar.no.endere√ßo.informadoÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•",
            "Endere√ßo.incompletoÂú∞ÂùÄ‰ø°ÊÅØ‰∏çËØ¶",
            "Impossibilidade.de.chegar.no.endere√ßo.informado.de.coleta.ÂÆ¢Êà∑Âú∞ÂùÄÊó†Ê≥ïËøõÂÖ•C"
        ])),
         "status": "ATEN√á√ÉO: AGUARDANDO DEVOLU√á√ÉO (ENDERE√áO/ACESSO INCORRETO)"},
    ]

    # Tentativas / aus√™ncia
    regras += [
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] ==
                                       "Aus√™ncia.de.destinat√°rio.nas.v√°rias.tentativas.de.entregaÂ§öÊ¨°Ê¥æÈÄÅÂÆ¢Êà∑‰∏çÂú®"),
         "status": "VERIFICAR 3 TENTATIVAS DE ENTREGA. SE OK, SOLICITAR DEVOLU√á√ÉO. SEN√ÉO, REALIZAR NOVA TENTATIVA."},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Aus√™ncia.do.destinat√°rioÂÆ¢Êà∑‰∏çÂú®") &
                     (df[COL_DIAS_PARADO] >= 2),
         "status": "ATEN√á√ÉO: DEVOLVER √Ä BASE (AUS√äNCIA, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Aus√™ncia.do.destinat√°rioÂÆ¢Êà∑‰∏çÂú®"),
         "status": "ATEN√á√ÉO: DEVOLU√á√ÉO √Ä BASE PENDENTE (AUS√äNCIA)"},
    ]

    # Recusa / mudan√ßa
    regras += [
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin([
            "Recusa.de.recebimento.pelo.cliente.(destinat√°rio)Êó†ÁêÜÁî±ÊãíÊî∂",
            "O.destinat√°rio.mudou.o.endere√ßo.Êî∂‰ª∂‰∫∫Êê¨ÂÆ∂"
        ])) & (df[COL_DIAS_PARADO] >= 2),
         "status": "ATEN√á√ÉO: DEVOLVER √Ä BASE (RECUSA/MUDAN√áA DE ENDERE√áO, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin([
            "Recusa.de.recebimento.pelo.cliente.(destinat√°rio)Êó†ÁêÜÁî±ÊãíÊî∂",
            "O.destinat√°rio.mudou.o.endere√ßo.Êî∂‰ª∂‰∫∫Êê¨ÂÆ∂"
        ])),
         "status": "ATEN√á√ÉO: DEVOLU√á√ÉO √Ä BASE PENDENTE (RECUSA/MUDAN√áA DE ENDERE√áO)"},
    ]

    # Outros problem√°ticos
    regras += [
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO].isin([
            "Pacote.fora.do.padr√£o.‰∏âËæπÂ∞∫ÂØ∏Ë∂ÖÈôê",
            "Embalagem.n√£o.conforme.ÂåÖË£Ö‰∏çËßÑËåÉ"
        ])),
         "status": "SOLICITAR DEVOLU√á√ÉO IMEDIATA (FORA DO PADR√ÉO / EMBALAGEM N√ÉO CONFORME)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Mercadorias.que.chegam.incompletosË¥ßÊú™Âà∞ÈΩê") &
                     (df[COL_DIAS_PARADO] >= 2),
         "status": "ENVIAR PARA O FLUXO INVERSO (INCOMPLETO, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Pacotes.retidos.por.anomalias.ÂºÇÂ∏∏Êã¶Êà™‰ª∂") &
                     (df[COL_DIAS_PARADO] >= 3),
         "status": "ENVIAR PARA A QUALIDADE (ANOMALIA, H√Å MAIS DE 3 DIAS)"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Pacotes.retidos.por.anomalias.ÂºÇÂ∏∏Êã¶Êà™‰ª∂"),
         "status": "ATEN√á√ÉO: ANOMALIA EM AN√ÅLISE"},
        {"condicao": is_problematico & (df[COL_NOME_PROBLEMATICO] == "Devolu√ß√£o.ÈÄÄÂõû‰ª∂"),
         "status": "ENVIAR PARA SC/DC (DEVOLU√á√ÉO APROVADA)"},
    ]

    # 2) OPERA√á√ïES NORMAIS
    regras += [
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega") &
                     (df[COL_REGIONAL].isin(FRANQUIAS)) & (df[COL_DIAS_PARADO] >= 2),
         "status": "ATRASO NA ENTREGA (FRANQUIA)"},
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega") &
                     (df[COL_REGIONAL].isin(FRANQUIAS)),
         "status": "EM ROTA DE ENTREGA (FRANQUIA)"},
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega") &
                     (~df[COL_REGIONAL].isin(FRANQUIAS)) & (df[COL_DIAS_PARADO] >= 2),
         "status": "ATEN√á√ÉO: ATRASO NA ENTREGA (BASE PR√ìPRIA)"},
        {"condicao": (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega"),
         "status": "EM ROTA DE ENTREGA (BASE PR√ìPRIA)"},
    ]

    # 3) ENVIO ERRADO (CDs)
    regras += [
        {"condicao": is_envio_errado_cd &
                     (df[COL_NOME_PROBLEMATICO] == "Mercadorias.do.cliente.n√£o.est√£o.completas.ÂÆ¢Êà∑Ë¥ßÁâ©Êú™Â§áÈΩê"),
         "status": "ENVIAR PARA O FLUXO INVERSO (INCOMPLETO, H√Å MAIS DE 2 DIAS)"},
        {"condicao": is_envio_errado_cd & (df[COL_NOME_PROBLEMATICO] == "Aus√™ncia.do.destinat√°rioÂÆ¢Êà∑‰∏çÂú®"),
         "status": "VERIFICAR 3 TENTATIVAS DE ENTREGA. SE OK, SOLICITAR DEVOLU√á√ÉO. SEN√ÉO, REALIZAR NOVA TENTATIVA."},
        {"condicao": is_envio_errado_cd,
         "status": "ENVIO ERRADO - ENTRE CDs"},
    ]

    conditions = [r["condicao"] for r in regras]
    choices = [r["status"] for r in regras]

    df[COL_STATUS] = np.select(
        conditions,
        choices,
        default=df[COL_ULTIMA_OPERACAO].astype(str).str.upper()
    )

    logging.info("Regras aplicadas com sucesso.")
    return df


def calcular_multa(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        logging.info("Nenhum pacote com 6+ dias para c√°lculo de multa.")
        return df

    logging.info("Calculando multa para pacotes com 6 ou mais dias parados...")
    df_copy = df.copy()

    condicoes_multa = [
        df_copy[COL_DIAS_PARADO] >= 30,
        df_copy[COL_DIAS_PARADO].between(14, 29),
        df_copy[COL_DIAS_PARADO].between(10, 13),
        df_copy[COL_DIAS_PARADO].between(7, 9),
        df_copy[COL_DIAS_PARADO] == 6,
        df_copy[COL_DIAS_PARADO] == 5,
        df_copy[COL_DIAS_PARADO] == 4,
        df_copy[COL_DIAS_PARADO] == 3,
        df_copy[COL_DIAS_PARADO] == 2,
    ]
    valores_multa = [30, 14, 10, 7, 6, 5, 4, 3, 2]

    df_copy[COL_MULTA] = np.select(condicoes_multa, valores_multa, default=0)
    logging.info("Multa calculada por item.")
    return df_copy
def processar_dados(df_main: pd.DataFrame, df_problematicos: pd.DataFrame, df_devolucao: pd.DataFrame) -> pd.DataFrame:
    colunas_necessarias = [
        COL_REMESSA, COL_ULTIMA_OPERACAO, COL_REGIONAL, COL_NOME_PROBLEMATICO, COL_HORA_OPERACAO, COL_BASE_RECENTE
    ]
    if not all(col in df_main.columns for col in colunas_necessarias):
        colunas_faltantes = set(colunas_necessarias) - set(df_main.columns)
        logging.critical(f"O arquivo principal n√£o cont√©m as colunas obrigat√≥rias: {colunas_faltantes}.")
        return pd.DataFrame()

    df = df_main.copy()
    df[COL_REMESSA] = df[COL_REMESSA].astype(str)

    # 1) Problem√°ticos
    if not df_problematicos.empty:
        if 'N√∫mero de pedido JMS' in df_problematicos.columns:
            df_problematicos['N√∫mero de pedido JMS'] = df_problematicos['N√∫mero de pedido JMS'].astype(str)

        if 'Tempo de digitaliza√ß√£o' in df_problematicos.columns:
            df_problematicos['Tempo de digitaliza√ß√£o'] = pd.to_datetime(df_problematicos['Tempo de digitaliza√ß√£o'], errors='coerce')

        df_problematicos = df_problematicos.sort_values('Tempo de digitaliza√ß√£o').dropna(subset=['N√∫mero de pedido JMS'])

        summary = df_problematicos.groupby('N√∫mero de pedido JMS').agg(
            Qtd_Problematicas=('N√∫mero de pedido JMS', 'size'),
            Ultima_Problematica_Detalhada=('Tipo de n√≠vel II de pacote problem√°tico', 'last')
        ).reset_index()

        df = df.merge(summary, left_on=COL_REMESSA, right_on='N√∫mero de pedido JMS', how='left')
        df.drop(columns=['N√∫mero de pedido JMS'], inplace=True, errors='ignore')
        logging.info("Dados de pacotes problem√°ticos integrados.")
    else:
        logging.info("Sem dados de pacotes problem√°ticos para integrar.")

    df['√öltima Problem√°tica Detalhada'] = df.get('Ultima_Problematica_Detalhada', pd.Series(index=df.index)).fillna('-')
    df['Qtd Problem√°ticas'] = df.get('Qtd_Problematicas', pd.Series(index=df.index)).fillna(0).astype(int)

    # 2) Devolu√ß√£o
    df[COL_DEVOLUCAO] = 'DEVOLU√á√ÉO N√ÉO SOLICITADA'
    if not df_devolucao.empty and 'N√∫mero de pedido JMS' in df_devolucao.columns:
        df_devolucao['N√∫mero de pedido JMS'] = df_devolucao['N√∫mero de pedido JMS'].astype(str)

        mapa_traducao = {'ÂæÖÂÆ°Ê†∏': 'EM PROCESSO DE APROVA√á√ÉO', 'È©≥Âõû': 'PEDIDO DE DEVOLU√á√ÉO RECUSADO', 'Â∑≤ÂÆ°Ê†∏': 'DEVOLU√á√ÉO APROVADA'}
        if 'Estado de solicita√ß√£o' in df_devolucao.columns:
            df_devolucao['Status_Traduzido'] = df_devolucao['Estado de solicita√ß√£o'].map(mapa_traducao)

            df_devolucao_info = (
                df_devolucao.dropna(subset=['Status_Traduzido'])[['N√∫mero de pedido JMS', 'Status_Traduzido']]
                .drop_duplicates(subset='N√∫mero de pedido JMS', keep='last')
            )

            df = df.merge(df_devolucao_info, left_on=COL_REMESSA, right_on='N√∫mero de pedido JMS', how='left')
            df[COL_DEVOLUCAO] = df['Status_Traduzido'].fillna(df[COL_DEVOLUCAO])
            df.drop(columns=['N√∫mero de pedido JMS', 'Status_Traduzido'], inplace=True, errors='ignore')
            logging.info("Dados de devolu√ß√£o integrados.")
    else:
        logging.info("Sem dados de devolu√ß√£o para integrar.")

    # 3) Dias Parado
    logging.info("Calculando dias parados...")
    df[COL_HORA_OPERACAO] = pd.to_datetime(df[COL_HORA_OPERACAO], errors='coerce')
    df[COL_DIAS_PARADO] = (datetime.now() - df[COL_HORA_OPERACAO]).dt.days.fillna(0).astype(int)

    # 4) Regras
    df = aplicar_regras_status(df)
    df = aplicar_regras_transito(df)

    # 5) Prioriza√ß√£o devolu√ß√£o
    df.loc[df[COL_DEVOLUCAO] != 'DEVOLU√á√ÉO N√ÉO SOLICITADA', COL_STATUS] = df[COL_DEVOLUCAO]
    cond_aprovado_em_rota = (df[COL_STATUS] == 'DEVOLU√á√ÉO APROVADA') & (df[COL_ULTIMA_OPERACAO] == "Âá∫‰ªìÊâ´Êèè/Bipe de sa√≠da para entrega")
    df.loc[cond_aprovado_em_rota, COL_STATUS] = 'DEVOLU√á√ÉO APROVADA, MAS O PACOTE EST√Å EM ROTA'

    condicao_aprovado = df[COL_STATUS].isin(['DEVOLU√á√ÉO APROVADA', 'DEVOLU√á√ÉO APROVADA, MAS O PACOTE EST√Å EM ROTA'])
    df.loc[condicao_aprovado, COL_TRANSITO] = ''

    df.rename(columns={COL_REGIONAL: COLUNA_CHAVE_PRINCIPAL}, inplace=True)

    ordem_colunas = [
        COL_REMESSA, COLUNA_CHAVE_PRINCIPAL, COL_DIAS_PARADO, COL_ULTIMA_OPERACAO,
        COL_HORA_OPERACAO, COL_STATUS, COL_TRANSITO, COL_DEVOLUCAO,
        'Qtd Problem√°ticas', '√öltima Problem√°tica Detalhada'
    ]
    colunas_existentes = [col for col in df.columns if col not in ordem_colunas]
    df = df[ordem_colunas + colunas_existentes]

    return df


def adicionar_info_coordenador(df_principal: pd.DataFrame) -> pd.DataFrame:
    if df_principal.empty:
        logging.warning("DataFrame de entrada est√° vazio. Pulando adi√ß√£o de coordenadores.")
        return df_principal

    try:
        logging.info(f"Lendo arquivo de mapeamento: {os.path.basename(ARQUIVO_MAPEAMENTO_COORDENADORES)}")
        df_mapeamento = pd.read_excel(ARQUIVO_MAPEAMENTO_COORDENADORES)
    except FileNotFoundError:
        logging.error(f"ERRO CR√çTICO: Arquivo de mapeamento '{ARQUIVO_MAPEAMENTO_COORDENADORES}' n√£o encontrado.")
        raise
    except Exception as e:
        logging.error(f"Ocorreu um erro ao ler o arquivo de mapeamento: {e}.")
        raise

    mapa_coordenador = pd.Series(df_mapeamento[COLUNA_INFO_COORDENADOR].values,
                                 index=df_mapeamento[COLUNA_CHAVE_MAPEAMENTO]).to_dict()
    mapa_filial = pd.Series(df_mapeamento[COLUNA_INFO_FILIAL].values,
                            index=df_mapeamento[COLUNA_CHAVE_MAPEAMENTO]).to_dict()

    key_series = df_principal[COLUNA_CHAVE_PRINCIPAL]
    if isinstance(key_series, pd.DataFrame):
        logging.warning(f"Colunas duplicadas para '{COLUNA_CHAVE_PRINCIPAL}'. Usando a primeira ocorr√™ncia.")
        key_series = key_series.iloc[:, 0]

    df_principal[NOVA_COLUNA_COORDENADOR] = key_series.map(mapa_coordenador).fillna('N√ÉO ENCONTRADO')
    df_principal[NOVA_COLUNA_FILIAL] = key_series.map(mapa_filial).fillna('N√ÉO ENCONTRADA')

    logging.info("Informa√ß√µes de coordenador e filial adicionadas.")
    return df_principal
def _mask_incompletos(df: pd.DataFrame) -> pd.Series:
    """
    M√°scara robusta para detectar "Mercadorias incompletas" na coluna Nome de pacote problem√°tico.
    """
    if df.empty or COL_NOME_PROBLEMATICO not in df.columns:
        return pd.Series([False] * len(df), index=df.index)

    s = df[COL_NOME_PROBLEMATICO].astype(str).fillna("").str.strip()
    return (
        (s == INCOMPLETOS_KEY_EXATA) |
        (s.str.contains(INCOMPLETOS_KEY_PARTE_1, na=False)) |
        (s.str.contains(INCOMPLETOS_KEY_PARTE_2, na=False))
    )


def salvar_relatorios(df_final: pd.DataFrame, pasta_saida: str) -> pd.DataFrame:
    """
    - Salva relat√≥rio separado de Mercadorias incompletas
    - REMOVE incompletos do relat√≥rio principal (0-4 / 5+)
    - Retorna df_principal (j√° sem incompletos) para usar no card do Feishu
    """
    if df_final.empty:
        logging.warning("‚ö†Ô∏è Nenhum dado para salvar relat√≥rios.")
        return df_final

    data_hoje = datetime.now().strftime("%Y-%m-%d")

    # 0) Separa INCOMPLETOS e remove do principal
    mask_incompletos = _mask_incompletos(df_final)
    df_incompletos = df_final[mask_incompletos].copy()
    df_principal = df_final[~mask_incompletos].copy()

    # 1) Salva relat√≥rio de INCOMPLETOS (se houver)
    if not df_incompletos.empty:
        arquivo_incompletos = os.path.join(pasta_saida, f"Relat√≥rio Mercadorias incompletas_{data_hoje}.xlsx")
        df_incompletos.to_excel(arquivo_incompletos, index=False)
        logging.info(f"‚úÖ Relat√≥rio Mercadorias incompletas salvo: {arquivo_incompletos}")
    else:
        logging.info("Sem registros de Mercadorias incompletas para salvar.")

    # üîç Debug: resumo por faixa de dias (AGORA no principal, sem incompletos)
    try:
        resumo = {
            "0-4 dias (principal)": (df_principal[COL_DIAS_PARADO] <= 4).sum(),
            "5+ dias (principal)": (df_principal[COL_DIAS_PARADO] >= 5).sum(),
            "Total (principal)": len(df_principal),
            "Incompletos (separado)": len(df_incompletos)
        }
        logging.info(f"üìä Resumo Dias Parados: {resumo}")
    except Exception as e:
        logging.error(f"Erro ao gerar resumo de debug: {e}")

    # 2) Relat√≥rio 0‚Äì4 dias (SEM incompletos)
    df_0_4 = df_principal[df_principal[COL_DIAS_PARADO] <= 4]
    if not df_0_4.empty:
        arquivo_0_4 = os.path.join(pasta_saida, f"Relat√≥rio Sem Movimenta√ß√£o (0-4 dias)_{data_hoje}.xlsx")
        df_0_4.to_excel(arquivo_0_4, index=False)
        logging.info(f"Relat√≥rio 0-4 dias salvo: {arquivo_0_4}")

    # 3) Relat√≥rio 5+ dias (SEM incompletos)
    df_5_plus = df_principal[df_principal[COL_DIAS_PARADO] >= 5]
    if not df_5_plus.empty:
        df_5_plus = calcular_multa(df_5_plus)
        arquivo_5_plus = os.path.join(pasta_saida, f"Relat√≥rio Sem Movimenta√ß√£o (5+ dias)_{data_hoje}.xlsx")
        df_5_plus.to_excel(arquivo_5_plus, index=False)
        logging.info(f"‚úÖ Relat√≥rio 5+ dias salvo: {arquivo_5_plus}")
    else:
        logging.warning("‚ö†Ô∏è Nenhum pedido encontrado com 5+ dias parados (principal).")

    # Retorna o principal j√° filtrado -> use isso no FEISHU CARD
    return df_principal


def mover_para_arquivo_morto(pasta_origem: str, pasta_destino: str):
    if not os.path.exists(pasta_destino):
        os.makedirs(pasta_destino)

    hoje = datetime.now().strftime("%Y-%m-%d")
    arquivos = [f for f in os.listdir(pasta_origem) if f.endswith(('.xlsx', '.xls'))]

    arquivos_hoje = [f for f in arquivos if hoje in f]
    arquivos_hoje.sort(key=lambda f: os.path.getmtime(os.path.join(pasta_origem, f)))

    if len(arquivos_hoje) > 1:
        for arquivo in arquivos_hoje[:-1]:
            try:
                shutil.move(os.path.join(pasta_origem, arquivo), os.path.join(pasta_destino, arquivo))
                logging.info(f"üì¶ Arquivo duplicado de hoje movido: {arquivo}")
            except Exception as e:
                logging.error(f"Erro ao mover o arquivo {arquivo}: {e}")

    for arquivo in arquivos:
        if arquivo not in arquivos_hoje:
            try:
                shutil.move(os.path.join(pasta_origem, arquivo), os.path.join(pasta_destino, arquivo))
                logging.info(f"üì¶ Arquivo antigo movido para Arquivo Morto: {arquivo}")
            except Exception as e:
                logging.error(f"Erro ao mover o arquivo {arquivo}: {e}")
def main():
    logging.info("--- INICIANDO PROCESSO DE GERA√á√ÉO DE RELAT√ìRIOS ---")

    caminho_arquivo_original = encontrar_arquivo_principal(PATH_INPUT_MAIN, FILENAME_START_MAIN)
    if not caminho_arquivo_original:
        logging.critical("Arquivo principal n√£o encontrado. Processo interrompido.")
        return

    df_main = pd.read_excel(caminho_arquivo_original)
    df_problematicos = carregar_planilhas_de_pasta(PATH_INPUT_PROBLEMATICOS, "Consolidando problem√°ticos")
    df_devolucao = carregar_planilhas_de_pasta(PATH_INPUT_DEVOLUCAO, "Consolidando devolu√ß√µes")

    df_final = processar_dados(df_main, df_problematicos, df_devolucao)
    df_final = adicionar_info_coordenador(df_final)

    # Move relat√≥rios antigos antes de salvar novos
    mover_para_arquivo_morto(PATH_OUTPUT_REPORTS, PATH_OUTPUT_ARQUIVO_MORTO)

    # Salva relat√≥rios e pega o DF PRINCIPAL (SEM incompletos) para o Feishu
    df_para_feishu = salvar_relatorios(df_final, PATH_OUTPUT_REPORTS)

    # ‚úÖ Use df_para_feishu para montar o card do Feishu (n√£o vai mais levar "incompletos")
    logging.info(f"DF para Feishu (principal, sem incompletos): {len(df_para_feishu)} linhas")

    logging.info("--- PROCESSO CONCLU√çDO COM SUCESSO! ---")


if __name__ == "__main__":
    main()
